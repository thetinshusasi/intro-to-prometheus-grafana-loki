Promethus : Time series db for query, storing data
Amazon cloud watch : ?

Prometheus  web interface can be accessed using https only. Also grafana also checks for certificates

Exporter : Tool used by Prometheus to pull metrics from various services such as DBs , SQL, IOT device, HAProxy
            Eg Node exporters
Push Gateway : Tool used by backend apps to push metrics on to and then it will be pulled
                 by Prometheus in sometime

Node Exporter is an agent designed to collect hardware and operating system metrics from a server and
expose them in a format that Prometheus can understand. It's a key component in the Prometheus
monitoring ecosystem, acting as the bridge between your machine's low-level data and the Prometheus server.


How it Works âš™ï¸
The Prometheus server uses a pull-based model for monitoring, which means it actively requests metrics from its targets. Node Exporter serves as one of these targets. You install Node Exporter on each server you want to monitor. Once it's running, it exposes a /metrics endpoint, typically on port 9100.


When configured, the Prometheus server periodically scrapes this endpoint, collecting a wide range of metrics about the host machine.

Key Functions ğŸ“Š
Node Exporter provides a wealth of information about the host it's running on, including:

CPU usage: Tracks total CPU time, as well as time spent in various modes like user, system, and I/O wait.

Memory utilization: Provides details on available memory, free memory, and swap usage.

Disk I/O and space: Monitors disk reads and writes, as well as filesystem capacity and available space.

Network statistics: Collects metrics on network traffic, including bytes sent and received, packet errors, and connection states.

System metrics: Gathers information on system uptime, boot time, and process counts.

- Node Exporter can be extended with pluggable metrics collector




Install Node exporter (provided for linux env )
 wget https://github.com/prometheus/node_exporter/releases/download/v1.9.1/node_exporter-1.9.1.darwin-amd64.tar.gz

 Node exporter will run on 9100 port
 Health check link : http://localhost:9100/metrics


 Prometheus yml file is located in "/usr/local/etc"

 In Ubuntu, you create or run Node Exporter as a service . Coz if you run in terminal , it will die once you close the terminal


 Prmethus Model 
 - <Meteric Name> {key1=value1,key2=value2...}

 Prometheus Data types
  - Scalar : float ,string
  - Instant Vectors : An instant vector is a set of time series, each containing one sample (value) per series at a specific instant in time.
    Instant vectors are â€œsnapshotsâ€ of your system at a single moment in time (the query evaluation time).
    Eg up
    might return
    up{instance="web-1", job="api"}  1
    up{instance="web-2", job="api"}  0
    
        - common examples
            http_requests_total
            node_cpu_seconds_total
            memory_usage_bytes
  - A range vector is a set of time series where each series includes multiple data points over a time range.


  We have binary (< , == , >  etc) and arthematic operateor (+ ,- , /, * ,%,^)


Matcher and selectors

   1. A metric selector tells Prometheus which metric name and which series (labels) to fetch.

    Syntax:
    metric_name{label_matchers}
    
   2.  Matchers are conditions inside {} that filter which time series to include based on label names and values.

    Types of matchers:
        = exact match
        != not equal
        =~ regex match
        !~ regex not match

    Examples:
    node_cpu_seconds_total{mode="idle"}
    -> select CPU idle time across all nodes


Alerts in Prometheus
   they are usually kept at 10% of the point of chaos
   defined in Alerts Definition Yml file
   written in promql
   stored in prometheus server
   Need an alert manager to send notifications or emails in Promethus



Some Awesome Alert configs : https://samber.github.io/awesome-prometheus-alerts/rules#postgresql


Alert manager
 For mac system : we had to use Mac port to install alert managers. For some reason there wasn't a homebrew system is available


================================================================================
PROMETHEUS ALERTS, GROUPS, RULES & ALERTMANAGER - COMPREHENSIVE SUMMARY
================================================================================

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. ALERT GROUPS - WHAT THEY ARE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is a Group?
- A group is a logical container for related alert rules
- Helps organize alerts by category, priority, or service
- All rules in a group are evaluated together at the same time
- Each group can have its own evaluation interval (optional)

Structure:
  groups:
    - name: GroupName
      interval: 30s  # Optional: how often to evaluate this group
      rules:
        - alert: AlertName1
        - alert: AlertName2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. MULTIPLE GROUPS & MULTIPLE RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Can You Have Multiple Groups?
âœ“ YES - You can have multiple groups in the same file or across multiple files

Can You Have Multiple Rules in a Group?
âœ“ YES - A group can contain multiple rules

Example:
  groups:
    - name: InfrastructureAlerts
      rules:
        - alert: NodeExporterDown
        - alert: HighCPUUsage
        - alert: DiskFull
    
    - name: ApplicationAlerts
      rules:
        - alert: ServiceDown
        - alert: HighLatency

Best Practices:
- Organize by category (Infrastructure, Application, System Resources)
- Use different intervals for different priorities
- Group related alerts together for easier management

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. HOW RULES WORK - INDEPENDENT EVALUATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key Point: Rules are evaluated INDEPENDENTLY

What This Means:
- Each rule in a group is evaluated separately
- If one rule fires, it doesn't affect other rules
- Each rule creates its own alert when it fires
- Non-firing rules don't create alerts

Example Scenario:
  Group: SystemAlerts
    - NodeExporterDown: FIRES â†’ Creates alert â†’ Sent to Alertmanager
    - HighCPU: Doesn't fire â†’ No alert created
    - HighMemory: FIRES â†’ Creates alert â†’ Sent to Alertmanager

Result: Two separate alerts are sent to Alertmanager

Important:
- Rules in a group are only evaluated together (same time)
- But each rule operates independently
- Each firing rule creates its own alert
- Alertmanager receives each alert separately

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. ALERTMANAGER - GROUPING & NOTIFICATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

How Alertmanager Groups Alerts:
- Alertmanager groups alerts by labels (like alertname, instance, severity)
- Default grouping: By 'alertname' only
- Grouping is controlled by 'group_by' setting in route configuration

Default Behavior:
  group_by: ['alertname']  # Default if not specified
  - All alerts with same alertname â†’ grouped together
  - Different alertnames â†’ separate groups

Custom Grouping Options:
  group_by: ['alertname', 'instance']  # Group by alertname AND instance
  group_by: ['severity']                # Group by severity only
  group_by: []                          # No grouping - one email per alert

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. EMAIL NOTIFICATIONS - HOW THEY WORK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

How Many Emails Are Sent?
- Alertmanager sends ONE EMAIL per alert group (not per alert)
- Multiple alerts in the same group â†’ ONE email with all alerts listed
- Different groups â†’ separate emails

Example: 3 identical alerts arrive
  - NodeExporterDown (server1)
  - NodeExporterDown (server2)
  - NodeExporterDown (server3)

Result: 1 EMAIL containing all 3 alerts separately

What's in the Email?
- Subject: Shows alert name and count [FIRING:3] NodeExporterDown
- Group labels: Common labels that define the group
- Individual alerts: Each alert listed separately with:
  * All its labels (instance, severity, job, etc.)
  * All its annotations (summary, description)
  * The metric value
  * Timestamp

Email Format Example:
  Subject: [FIRING:3] NodeExporterDown
  
  Alert Group: {alertname="NodeExporterDown", severity="critical"}
  
  Alerts (3):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Alert 1:
    Labels:
      - alertname: NodeExporterDown
      - instance: server1
      - severity: critical
    Annotations:
      - summary: Node Exporter is not running
      - description: Node Exporter is not running on server1
    Value: 0
  
  Alert 2:
    Labels:
      - alertname: NodeExporterDown
      - instance: server2
      ...
  
  Alert 3:
    Labels:
      - alertname: NodeExporterDown
      - instance: server3
      ...

Key Points:
âœ“ One email per group (not per alert)
âœ“ Each alert is listed separately with full details
âœ“ You can see which specific instances/servers are affected
âœ“ Subject shows count of alerts in the group

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. TIMING CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Alertmanager Route Timing Parameters:

group_wait: 10s
  - How long to wait before sending initial notification for a new group
  - If multiple alerts arrive within this time, they're grouped together
  
group_interval: 10s
  - How long to wait before sending a new notification about the same group
  - If new alerts join the group, wait this long before sending update
  
repeat_interval: 10m
  - How long to wait before repeating the same alert notification
  - Default is 4 hours if not specified
  - Controls how often you get reminded about the same alert

Example Timeline:
  T0:  3 alerts arrive â†’ Group starts
  T10s: group_wait expires â†’ First email sent (3 alerts)
  T20s: 1 new alert joins group â†’ group_interval starts
  T30s: group_interval expires â†’ Second email sent (4 alerts)
  T10m: repeat_interval expires â†’ Third email sent (still 4 alerts)

Best Practices:
- Critical alerts: Shorter intervals (group_wait: 5s, repeat_interval: 2m)
- Warning alerts: Longer intervals (group_wait: 30s, repeat_interval: 10m)
- Info alerts: Even longer intervals (group_wait: 1m, repeat_interval: 30m)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. PURPOSE OF MULTIPLE RULES IN A GROUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Why Have Multiple Rules in a Group?

1. Monitor Different Conditions
   - Each rule monitors different metrics or conditions
   - Example: ServiceDown, HighLatency, ErrorRateHigh, DiskFull

2. Different Severity Levels
   - Different rules can have different severity levels
   - Example: critical for ServiceDown, warning for HighCPU

3. Different Thresholds for Same Metric
   - Multiple rules for same metric with different thresholds
   - Example: CPUWarning (>70%), CPUCritical (>90%)

4. Different Time Windows
   - Different rules can have different 'for' durations
   - Example: ServiceDown (30s), ServiceDegraded (5m)

5. Different Routing
   - Different labels allow different routing in Alertmanager
   - Example: severity=critical â†’ urgent_receiver, severity=warning â†’ main_receiver

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. LABEL EVALUATION - WHEN RULES DON'T FIRE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When Expression is FALSE:
- Alert is in "inactive" state
- Labels and annotations are NOT evaluated
- Template variables ({{ $labels.instance }}) are not populated
- This is by design - Prometheus doesn't waste resources

When Expression is TRUE:
- Alert is in "pending" or "firing" state
- Labels and annotations ARE evaluated
- Template variables are populated with actual values

Key Point: Labels and annotations are only evaluated when the alert is firing

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. COMPLETE WORKFLOW EXAMPLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: System with 3 alerts configured

Step 1: Prometheus Evaluates Rules
  - Group: SystemAlerts
    * NodeExporterDown: FIRES (up == 0 for 1m)
    * HighCPU: Doesn't fire (CPU is 60%)
    * HighMemory: FIRES (memory is 95%)

Step 2: Prometheus Creates Alerts
  - Alert 1: NodeExporterDown (severity: critical, instance: server1)
  - Alert 2: HighMemory (severity: warning, instance: server1)
  - No alert for HighCPU

Step 3: Alerts Sent to Alertmanager
  - NodeExporterDown â†’ Alertmanager
  - HighMemory â†’ Alertmanager

Step 4: Alertmanager Groups Alerts
  - Group 1: {alertname="NodeExporterDown", severity="critical"}
    â†’ Routes to urgent_receiver (matches severity=critical)
  - Group 2: {alertname="HighMemory", severity="warning"}
    â†’ Routes to main_receiver (default)

Step 5: Email Notifications Sent
  - Email 1: Critical alert for NodeExporterDown
  - Email 2: Warning alert for HighMemory

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Rules are evaluated independently - each rule operates separately
âœ“ Only firing rules create alerts - non-firing rules don't create alerts
âœ“ Each alert is sent individually to Alertmanager
âœ“ Alertmanager groups alerts by labels for notifications
âœ“ Multiple alerts in same group â†’ ONE email with all alerts listed separately
âœ“ Each alert in email shows full details (labels, annotations, values)
âœ“ Timing configuration controls how often notifications are sent
âœ“ Labels/annotations only evaluated when alert is firing
âœ“ Groups help organize related alerts together
âœ“ Multiple rules allow monitoring different conditions, severities, thresholds

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
INHIBITION RULES & SILENCING IN ALERTMANAGER - COMPREHENSIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. INHIBITION RULES - OVERVIEW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Inhibition?
- Inhibition rules suppress certain alerts when other alerts are firing
- Prevents alert flooding by hiding less important alerts when critical ones fire
- Example: If "ServerDown" is firing, suppress "HighCPU" alerts for that server
- Configured in Alertmanager configuration file (alertmanager.yml)

Key Concept:
  Source Alert (firing) â†’ Inhibits â†’ Target Alerts (suppressed)
  
  When source alert fires, target alerts matching the rule are suppressed
  Target alerts are not sent to receivers while source alert is active

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. INHIBITION RULE SYNTAX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Structure:
  inhibit_rules:
    - source_match:          # Source alert that triggers inhibition
        label_name: value
      target_match:          # Target alerts to be suppressed
        label_name: value
      equal: ['label1', 'label2']  # Labels that must match between source and target

Components:
  source_match: Conditions that identify the source alert (the inhibitor)
  target_match: Conditions that identify target alerts (to be suppressed)
  equal: Labels that must have the same value in both source and target

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. INHIBITION RULE EXAMPLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example 1: Suppress Warnings When Critical Alerts Fire
  inhibit_rules:
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'instance']
  
  Meaning:
    - When any critical alert fires
    - Suppress warning alerts with same alertname and instance
    - Prevents getting both critical and warning alerts for same issue

Example 2: Suppress All Alerts When Server is Down
  inhibit_rules:
    - source_match:
        alertname: ServerDown
      target_match:
        severity: warning
      equal: ['instance']
  
  Meaning:
    - When ServerDown alert fires for an instance
    - Suppress all warning alerts for that same instance
    - Makes sense: if server is down, no point alerting about high CPU

Example 3: Suppress Specific Alerts When Parent Alert Fires
  inhibit_rules:
    - source_match:
        alertname: ClusterDown
      target_match:
        alertname: NodeHighCPU
      equal: ['cluster']
  
  Meaning:
    - When ClusterDown fires
    - Suppress NodeHighCPU alerts in the same cluster
    - Cluster-level issue makes node-level alerts irrelevant

Example 4: Multiple Source Conditions
  inhibit_rules:
    - source_match:
        severity: critical
        component: database
      target_match:
        severity: warning
        component: database
      equal: ['instance']
  
  Meaning:
    - When critical database alert fires
    - Suppress warning database alerts for same instance

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. HOW INHIBITION WORKS - STEP BY STEP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Alerts Arrive at Alertmanager
  - Alert A: ServerDown (severity: critical, instance: server1)
  - Alert B: HighCPU (severity: warning, instance: server1)
  - Alert C: HighMemory (severity: warning, instance: server1)

Step 2: Inhibition Rules Evaluated
  Rule: source_match {severity: critical} â†’ target_match {severity: warning}
        equal: ['instance']
  
  Check:
    - Source alert (ServerDown) matches source_match? YES (severity: critical)
    - Target alerts (HighCPU, HighMemory) match target_match? YES (severity: warning)
    - Equal labels match? YES (instance: server1 for all)

Step 3: Target Alerts Suppressed
  - Alert A (ServerDown): NOT suppressed (it's the source)
  - Alert B (HighCPU): SUPPRESSED (matches target_match and equal)
  - Alert C (HighMemory): SUPPRESSED (matches target_match and equal)

Step 4: Only Source Alert Sent
  - Only ServerDown alert is sent to receivers
  - HighCPU and HighMemory are suppressed (not sent)

Step 5: When Source Alert Resolves
  - ServerDown alert resolves (server comes back up)
  - Inhibition stops
  - HighCPU and HighMemory alerts can now fire again if conditions still exist

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. SILENCING - OVERVIEW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Silencing?
- Silencing is a manual way to suppress alerts temporarily
- Done through Alertmanager UI or API
- Useful for planned maintenance, known issues, or testing
- More flexible than inhibition rules (can be created/removed on demand)

Key Differences from Inhibition:
  Inhibition Rules:
    - Automatic (configured in alertmanager.yml)
    - Based on alert conditions (source/target matching)
    - Always active when conditions are met
    - Requires configuration change to modify
  
  Silencing:
    - Manual (created through UI/API)
    - Based on matchers (label selectors)
    - Can have time limits (start/end time)
    - Can be created/removed without config changes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. HOW TO CREATE SILENCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Method 1: Through Alertmanager UI
  1. Go to http://localhost:9093
  2. Click on "Silences" tab
  3. Click "New Silence" button
  4. Fill in:
     - Matchers: Label selectors (e.g., alertname="HighCPU", instance="server1")
     - Starts at: When silence should start
     - Ends at: When silence should end (or duration)
     - Created by: Your name/email
     - Comment: Reason for silence (e.g., "Planned maintenance")
  5. Click "Create"

Method 2: Through API
  POST http://localhost:9093/api/v2/silences
  
  Body:
  {
    "matchers": [
      {"name": "alertname", "value": "HighCPU", "isRegex": false},
      {"name": "instance", "value": "server1", "isRegex": false}
    ],
    "startsAt": "2025-11-08T10:00:00Z",
    "endsAt": "2025-11-08T12:00:00Z",
    "createdBy": "admin@example.com",
    "comment": "Planned maintenance window"
  }

Method 3: Using amtool (Command Line)
  amtool silence add \
    alertname=HighCPU instance=server1 \
    --start=2025-11-08T10:00:00Z \
    --end=2025-11-08T12:00:00Z \
    --comment="Planned maintenance"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. SILENCE MATCHERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Matchers are label selectors that determine which alerts to silence.

Types of Matchers:
  1. Exact Match:
     alertname="HighCPU"
     instance="server1"
  
  2. Regex Match:
     alertname=~"High.*"
     instance=~"server[0-9]+"
  
  3. Negative Match:
     alertname!="HighCPU"
     severity!="info"

Examples:
  Silence all HighCPU alerts:
    alertname="HighCPU"
  
  Silence all alerts on server1:
    instance="server1"
  
  Silence all critical alerts:
    severity="critical"
  
  Silence HighCPU on specific server:
    alertname="HighCPU"
    instance="server1"
  
  Silence all alerts matching pattern:
    alertname=~"High.*"
    instance=~"server[0-9]+"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. SILENCE DURATION & EXPIRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Time-Based Silencing:
  - Silences can have start and end times
  - Useful for planned maintenance windows
  - Automatically expires when end time is reached

Example:
  Starts at: 2025-11-08 10:00:00
  Ends at:   2025-11-08 12:00:00
  Duration:  2 hours
  
  - Alerts matching the silence are suppressed from 10:00 to 12:00
  - After 12:00, silence expires and alerts can fire again

Permanent Silences:
  - Can set end time far in the future
  - Or use very long duration
  - Remember to manually expire when no longer needed

Expiring Silences:
  - Silences automatically expire at their end time
  - Can also be manually expired through UI/API
  - Expired silences are kept in history for reference

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. INHIBITION VS SILENCING - COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature             â”‚ Inhibition Rules      â”‚ Silencing            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Configuration       â”‚ alertmanager.yml      â”‚ UI/API/amtool        â”‚
â”‚ Type                â”‚ Automatic             â”‚ Manual               â”‚
â”‚ Trigger             â”‚ Based on alert state  â”‚ Based on matchers    â”‚
â”‚ Duration            â”‚ While source fires    â”‚ Time-based or manual â”‚
â”‚ Flexibility         â”‚ Fixed rules           â”‚ Very flexible        â”‚
â”‚ Use Case            â”‚ Alert relationships   â”‚ Maintenance/testing  â”‚
â”‚ Modification         â”‚ Config change + reload  â”‚ Instant via UI      â”‚
â”‚ Scope               â”‚ Sourceâ†’Target logic    â”‚ Any matcher pattern  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When to Use Inhibition:
  âœ“ Automatic suppression based on alert relationships
  âœ“ Reduce alert noise when critical issues occur
  âœ“ Permanent rules for known alert dependencies
  âœ“ Example: Suppress warnings when critical alerts fire

When to Use Silencing:
  âœ“ Planned maintenance windows
  âœ“ Known issues being worked on
  âœ“ Testing alert configurations
  âœ“ Temporary suppression of noisy alerts
  âœ“ One-off situations

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. COMPLETE EXAMPLE - INHIBITION RULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration:
  inhibit_rules:
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'instance', 'job']

Scenario:
  Alerts Firing:
    1. ServerDown (severity: critical, instance: server1, job: web)
    2. HighCPU (severity: warning, instance: server1, job: web)
    3. HighMemory (severity: warning, instance: server1, job: web)
    4. HighCPU (severity: warning, instance: server2, job: web)

Evaluation:
  Source Match: ServerDown (severity: critical) âœ“
  
  Target Match Check:
    - HighCPU on server1: severity=warning âœ“, equal labels match âœ“ â†’ SUPPRESSED
    - HighMemory on server1: severity=warning âœ“, equal labels match âœ“ â†’ SUPPRESSED
    - HighCPU on server2: severity=warning âœ“, equal labels DON'T match âœ— â†’ NOT SUPPRESSED

Result:
  - ServerDown: Sent (source alert)
  - HighCPU on server1: Suppressed (matches target and equal)
  - HighMemory on server1: Suppressed (matches target and equal)
  - HighCPU on server2: Sent (equal labels don't match - different instance)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
11. COMPLETE EXAMPLE - SILENCING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: Planned maintenance on server1 from 10:00 to 12:00

Step 1: Create Silence
  Matchers:
    - instance="server1"
  
  Time:
    - Starts: 2025-11-08 10:00:00
    - Ends:   2025-11-08 12:00:00
  
  Comment: "Planned maintenance - server1"

Step 2: During Maintenance
  Alerts that would fire:
    - ServerDown (instance: server1) â†’ SUPPRESSED by silence
    - HighCPU (instance: server1) â†’ SUPPRESSED by silence
    - HighMemory (instance: server1) â†’ SUPPRESSED by silence
    - HighCPU (instance: server2) â†’ NOT suppressed (different instance)

Step 3: After Maintenance
  - Silence expires at 12:00
  - Alerts for server1 can fire again normally

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
12. BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Rules:
  âœ“ Keep rules simple and clear
  âœ“ Document why each rule exists
  âœ“ Test rules carefully before deploying
  âœ“ Use 'equal' labels to ensure proper matching
  âœ“ Don't over-inhibit - you might miss important alerts

Silencing:
  âœ“ Always add comments explaining why silence was created
  âœ“ Set appropriate end times (don't leave permanent silences)
  âœ“ Review and expire old silences regularly
  âœ“ Use specific matchers (don't silence too broadly)
  âœ“ Document maintenance windows in silence comments

General:
  âœ“ Use inhibition for automatic, permanent relationships
  âœ“ Use silencing for temporary, manual situations
  âœ“ Monitor silenced alerts to ensure they're appropriate
  âœ“ Review silence history periodically
  âœ“ Combine both: inhibition for logic, silencing for exceptions

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
13. COMMON USE CASES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Use Cases:
  1. Suppress warnings when critical alerts fire
  2. Suppress node-level alerts when cluster-level alert fires
  3. Suppress application alerts when infrastructure alert fires
  4. Suppress dependent alerts when root cause alert fires

Silencing Use Cases:
  1. Planned maintenance windows
  2. Known issues being actively worked on
  3. Testing alert configurations
  4. Suppressing noisy alerts temporarily
  5. Scheduled downtime periods
  6. Development/testing environments

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
14. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Rules:
  âœ“ Automatic suppression based on alert relationships
  âœ“ Configured in alertmanager.yml
  âœ“ Source alert suppresses target alerts
  âœ“ Uses source_match, target_match, and equal labels
  âœ“ Active while source alert is firing

Silencing:
  âœ“ Manual suppression via UI/API/amtool
  âœ“ Based on label matchers
  âœ“ Can be time-limited
  âœ“ More flexible than inhibition
  âœ“ Useful for temporary situations

Both:
  âœ“ Help reduce alert noise
  âœ“ Prevent alert flooding
  âœ“ Improve signal-to-noise ratio
  âœ“ Should be used thoughtfully
  âœ“ Need proper documentation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RECORDING RULES IN PROMETHEUS - COMPREHENSIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. WHAT ARE RECORDING RULES?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition:
- Recording rules pre-compute frequently used or expensive queries
- They store the result as a new time series with a new metric name
- Results are stored in Prometheus's time series database
- Can be queried like any other metric

Purpose:
- Improve query performance (pre-compute expensive calculations)
- Simplify complex queries (create reusable metrics)
- Reduce query load on Prometheus
- Create aggregated metrics for dashboards
- Standardize metric names across environments

Key Difference from Alert Rules:
  Alert Rules:
    - Evaluate conditions and create alerts
    - Don't store new metrics
    - Used for alerting only
  
  Recording Rules:
    - Evaluate expressions and store results as new metrics
    - Create new time series data
    - Used for querying and dashboards

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. RECORDING RULE SYNTAX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Structure:
  groups:
    - name: group_name
      interval: 30s  # Optional: evaluation interval
      rules:
        - record: new_metric_name
          expr: promql_expression

Components:
  record: The name of the new metric to create
  expr: The PromQL expression to evaluate
  labels: Optional labels to add to the new metric

Example:
  groups:
    - name: cpu_recording_rules
      interval: 30s
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. WHY USE RECORDING RULES?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Performance Optimization
   - Pre-compute expensive queries (aggregations, rate calculations)
   - Reduce query time for dashboards
   - Lower CPU usage on Prometheus server
   - Faster response times for frequently accessed metrics

2. Query Simplification
   - Complex expressions become simple metric queries
   - Example: Instead of:
       rate(http_requests_total[5m]) / rate(http_requests_total[5m]) * 100
     Use pre-computed: http_request_error_rate

3. Reusability
   - Define once, use everywhere
   - Consistent calculations across dashboards
   - Easier to maintain

4. Aggregation
   - Create cluster-level, service-level, or environment-level metrics
   - Aggregate across multiple instances
   - Create summary metrics

5. Standardization
   - Consistent metric naming conventions
   - Standardize metric names across different exporters
   - Create unified metrics from different sources

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. RECORDING RULE EXAMPLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example 1: CPU Usage Percentage
  groups:
    - name: cpu_recording_rules
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: |
            100 - (
              avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) 
              * 100
            )
  
  Usage: Query `instance:node_cpu_usage:rate5m` instead of complex expression

Example 2: Memory Usage Percentage
  groups:
    - name: memory_recording_rules
      rules:
        - record: instance:node_memory_usage:ratio
          expr: |
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
            / node_memory_MemTotal_bytes
  
  Usage: Query `instance:node_memory_usage:ratio` for memory usage

Example 3: HTTP Request Rate
  groups:
    - name: http_recording_rules
      rules:
        - record: http:requests:rate5m
          expr: rate(http_requests_total[5m])
  
  Usage: Query `http:requests:rate5m` instead of `rate(http_requests_total[5m])`

Example 4: Error Rate Percentage
  groups:
    - name: http_recording_rules
      rules:
        - record: http:request_error_rate:ratio
          expr: |
            rate(http_requests_total{status=~"5.."}[5m]) 
            / rate(http_requests_total[5m])
  
  Usage: Query `http:request_error_rate:ratio` for error percentage

Example 5: Aggregated Metrics
  groups:
    - name: cluster_recording_rules
      rules:
        - record: cluster:cpu_usage:avg
          expr: avg(instance:node_cpu_usage:rate5m)
          labels:
            cluster: production
        
        - record: cluster:memory_usage:avg
          expr: avg(instance:node_memory_usage:ratio)
          labels:
            cluster: production

Example 6: Multi-step Calculation
  groups:
    - name: service_recording_rules
      rules:
        # Step 1: Calculate request rate
        - record: service:http_requests:rate5m
          expr: rate(http_requests_total[5m])
        
        # Step 2: Calculate error rate using step 1
        - record: service:http_errors:rate5m
          expr: rate(http_requests_total{status=~"5.."}[5m])
        
        # Step 3: Calculate error percentage using steps 1 and 2
        - record: service:http_error_rate:ratio
          expr: |
            service:http_errors:rate5m 
            / service:http_requests:rate5m

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. NAMING CONVENTIONS FOR RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Best Practice Naming Pattern:
  level:metric:operation

Components:
  level: The aggregation level (instance, service, cluster, etc.)
  metric: The metric name being recorded
  operation: The operation performed (rate, sum, avg, ratio, etc.)

Examples:
  instance:node_cpu_usage:rate5m
    - level: instance (per-instance metric)
    - metric: node_cpu_usage
    - operation: rate5m (5-minute rate)
  
  cluster:http_requests:sum
    - level: cluster (cluster-wide)
    - metric: http_requests
    - operation: sum (summed across instances)
  
  service:memory_usage:avg
    - level: service (service-level)
    - metric: memory_usage
    - operation: avg (averaged)

Common Operations:
  :rate5m    - 5-minute rate
  :rate1m    - 1-minute rate
  :sum       - Summed value
  :avg       - Average value
  :max       - Maximum value
  :min       - Minimum value
  :ratio     - Ratio/percentage
  :increase  - Increase over time

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. HOW TO SAVE RECORDING RULES - FILE ORGANIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Best Practice: Separate Files by Purpose

Option 1: Separate Files (RECOMMENDED)
  Directory Structure:
    /usr/local/etc/rule/
      â”œâ”€â”€ alerts.yml          # Alert rules only
      â”œâ”€â”€ recording_cpu.yml    # CPU-related recording rules
      â”œâ”€â”€ recording_memory.yml # Memory-related recording rules
      â”œâ”€â”€ recording_http.yml  # HTTP-related recording rules
      â””â”€â”€ recording_cluster.yml # Cluster-level recording rules

  prometheus.yml:
    rule_files:
      - "/usr/local/etc/rule/alerts.yml"
      - "/usr/local/etc/rule/recording_cpu.yml"
      - "/usr/local/etc/rule/recording_memory.yml"
      - "/usr/local/etc/rule/recording_http.yml"
      - "/usr/local/etc/rule/recording_cluster.yml"

Option 2: Single File with Groups
  File: /usr/local/etc/rule/recording_rules.yml
  
  groups:
    - name: cpu_recording_rules
      rules: [...]
    - name: memory_recording_rules
      rules: [...]
    - name: http_recording_rules
      rules: [...]

  prometheus.yml:
    rule_files:
      - "/usr/local/etc/rule/alerts.yml"
      - "/usr/local/etc/rule/recording_rules.yml"

Option 3: By Service/Component
  Directory Structure:
    /usr/local/etc/rule/
      â”œâ”€â”€ alerts.yml
      â”œâ”€â”€ node_exporter_recording.yml
      â”œâ”€â”€ application_recording.yml
      â””â”€â”€ infrastructure_recording.yml

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. CONFIGURING RECORDING RULES IN PROMETHEUS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Create Recording Rules File
  Create file: /usr/local/etc/rule/recording_rules.yml
  
  groups:
    - name: cpu_recording_rules
      interval: 30s
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

Step 2: Add to prometheus.yml
  rule_files:
    - "/usr/local/etc/rule/alerts.yml"
    - "/usr/local/etc/rule/recording_rules.yml"

Step 3: Reload Prometheus
  curl -X POST http://localhost:9090/-/reload
  
  Or restart Prometheus service

Step 4: Verify Rules Are Loaded
  - Go to http://localhost:9090/rules
  - Check that recording rules appear
  - Verify they're being evaluated

Step 5: Query the New Metric
  - In Prometheus UI, query: instance:node_cpu_usage:rate5m
  - Should return time series data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. BEST PRACTICES FOR SAVING RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. File Organization
   âœ“ Separate recording rules from alert rules
   âœ“ Group related rules together
   âœ“ Use descriptive file names
   âœ“ Keep files focused and manageable size
   âœ“ Document what each file contains

2. Naming Conventions
   âœ“ Use consistent naming pattern (level:metric:operation)
   âœ“ Make names descriptive and self-documenting
   âœ“ Avoid generic names like "metric1", "metric2"
   âœ“ Include operation type in name (rate, sum, avg)
   âœ“ Use underscores, not hyphens or spaces

3. Group Organization
   âœ“ Group by metric type (CPU, memory, network)
   âœ“ Group by service/component
   âœ“ Group by aggregation level (instance, cluster)
   âœ“ Use descriptive group names
   âœ“ Keep groups focused on single purpose

4. Evaluation Intervals
   âœ“ Set appropriate intervals based on use case
   âœ“ Shorter intervals for frequently queried metrics
   âœ“ Longer intervals for less critical metrics
   âœ“ Consider query frequency when setting intervals
   âœ“ Default: Uses global evaluation_interval if not specified

5. Documentation
   âœ“ Add comments explaining complex expressions
   âœ“ Document why recording rule exists
   âœ“ Note dependencies between rules
   âœ“ Include examples of how to use the metric
   âœ“ Document any assumptions or limitations

6. Version Control
   âœ“ Store rules in version control (git)
   âœ“ Use same workflow as alert rules
   âœ“ Review changes before deploying
   âœ“ Test rules in development first
   âœ“ Keep history of changes

7. Testing
   âœ“ Test expressions in Prometheus UI first
   âœ“ Verify results match expectations
   âœ“ Check for errors in Prometheus logs
   âœ“ Monitor rule evaluation performance
   âœ“ Validate metric names are correct

8. Maintenance
   âœ“ Review unused recording rules periodically
   âœ“ Remove obsolete rules
   âœ“ Update rules when source metrics change
   âœ“ Monitor rule evaluation time
   âœ“ Optimize expensive rules

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. COMPLETE EXAMPLE - RECORDING RULES FILE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

File: /usr/local/etc/rule/recording_rules.yml

groups:
  # CPU-related recording rules
  - name: cpu_recording_rules
    interval: 30s
    rules:
      # CPU usage percentage (5-minute rate)
      - record: instance:node_cpu_usage:rate5m
        expr: |
          100 - (
            avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) 
            * 100
          )
      
      # CPU usage by mode
      - record: instance:node_cpu_seconds:rate5m
        expr: rate(node_cpu_seconds_total[5m])
  
  # Memory-related recording rules
  - name: memory_recording_rules
    interval: 30s
    rules:
      # Memory usage ratio
      - record: instance:node_memory_usage:ratio
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
          / node_memory_MemTotal_bytes
      
      # Memory usage percentage
      - record: instance:node_memory_usage:percent
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
          / node_memory_MemTotal_bytes 
          * 100
  
  # Disk-related recording rules
  - name: disk_recording_rules
    interval: 1m
    rules:
      # Disk usage ratio
      - record: instance:node_filesystem_usage:ratio
        expr: |
          (node_filesystem_size_bytes - node_filesystem_avail_bytes) 
          / node_filesystem_size_bytes
      
      # Disk usage percentage
      - record: instance:node_filesystem_usage:percent
        expr: |
          (node_filesystem_size_bytes - node_filesystem_avail_bytes) 
          / node_filesystem_size_bytes 
          * 100

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. INTEGRATION WITH YOUR CURRENT SETUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Current Setup:
  - Config files in: /usr/local/etc/
  - Alert rules in: /usr/local/etc/rule/alerts.yml
  - Repo tracking: config/rule/alerts.yml
  - Sync script: sync-config.sh

Recommended Addition:
  1. Create recording rules file:
     config/rule/recording_rules.yml
  
  2. Update prometheus.yml:
     rule_files:
       - "/usr/local/etc/rule/alerts.yml"
       - "/usr/local/etc/rule/recording_rules.yml"
  
  3. Update sync-config.sh to include recording rules:
     sudo cp config/rule/recording_rules.yml /usr/local/etc/rule/recording_rules.yml
  
  4. Sync and reload:
     ./sync-config.sh
     curl -X POST http://localhost:9090/-/reload

Directory Structure:
  config/
    â”œâ”€â”€ prometheus.yml
    â””â”€â”€ rule/
        â”œâ”€â”€ alerts.yml
        â””â”€â”€ recording_rules.yml

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
11. COMMON PATTERNS AND USE CASES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Pattern 1: Rate Calculations
  - Pre-compute rate() for frequently queried counters
  - Reduces query time for dashboards
  - Example: HTTP request rates, error rates

Pattern 2: Aggregations
  - Create cluster/service-level metrics
  - Aggregate across multiple instances
  - Example: Average CPU across cluster

Pattern 3: Percentages and Ratios
  - Convert raw values to percentages
  - Calculate ratios between metrics
  - Example: CPU usage %, error rate %

Pattern 4: Multi-step Calculations
  - Break complex calculations into steps
  - Each step creates a reusable metric
  - Example: Error rate = errors / total requests

Pattern 5: Standardization
  - Normalize metric names from different exporters
  - Create unified naming conventions
  - Example: Standardize CPU metric names

Pattern 6: Historical Aggregations
  - Pre-compute aggregations over time windows
  - Useful for long-term trends
  - Example: Daily/weekly averages

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
12. MONITORING RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Check Rule Status:
  - Prometheus UI: http://localhost:9090/rules
  - Shows all recording rules and their status
  - Indicates if rules are being evaluated

Check Rule Performance:
  - Prometheus UI: http://localhost:9090/rules
  - Shows evaluation time for each rule
  - Monitor for slow rules

Check Metrics Exist:
  - Query the recorded metric in Prometheus UI
  - Verify data is being stored
  - Check metric labels are correct

Common Issues:
  - Rules not loading: Check file path in prometheus.yml
  - No data: Check source metrics exist
  - Errors: Check Prometheus logs
  - Slow evaluation: Optimize expressions or increase interval

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
13. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Recording rules pre-compute and store query results as new metrics
âœ“ Use them to improve performance and simplify queries
âœ“ Follow naming convention: level:metric:operation
âœ“ Separate recording rules from alert rules in different files
âœ“ Organize rules by purpose (CPU, memory, HTTP, etc.)
âœ“ Set appropriate evaluation intervals
âœ“ Document complex expressions
âœ“ Test rules before deploying
âœ“ Monitor rule performance
âœ“ Keep rules in version control
âœ“ Use consistent file organization
âœ“ Group related rules together

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


 Implement Side Car patern


