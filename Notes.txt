================================================================================
                    OBSERVABILITY STACK ARCHITECTURE
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           APPLICATION LAYER                                  â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   App 1      â”‚    â”‚   App 2      â”‚    â”‚   App N      â”‚                 â”‚
â”‚  â”‚  (Metrics)   â”‚    â”‚  (Metrics)   â”‚    â”‚  (Metrics)   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚         â”‚                   â”‚                    â”‚                          â”‚
â”‚         â”‚ PUSH              â”‚ PUSH               â”‚ PUSH                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                                                                  â”‚            â”‚
â”‚                                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                                         â”‚  Push Gateway   â”‚  â”‚
â”‚                                                         â”‚  (Port 9091)    â”‚  â”‚
â”‚                                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                   â”‚
                                                                   â”‚ PULL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         METRICS COLLECTION                        â”‚           â”‚
â”‚                                                                   â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Prometheus Server                                 â”‚   â”‚
â”‚  â”‚                    (Port 9090)                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Service Discovery                                           â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  - Static configs                                             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  - File-based SD                                             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  - DNS-based SD                                               â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  - Kubernetes SD                                              â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Scrape Targets (PULL)                                       â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Push Gateway â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Node Exporter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”                         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ App endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”                      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Alloy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”                   â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚  â”‚  â”‚  â”‚                           â”‚
â”‚                                        â”‚  â”‚  â”‚  â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â–¼â”€â”€â–¼â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Node Exporter                                      â”‚  â”‚
â”‚  â”‚                    (Port 9100)                                       â”‚  â”‚
â”‚  â”‚                    â€¢ CPU, Memory, Disk, Network                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Alloy (OpenTelemetry Collector)                  â”‚  â”‚
â”‚  â”‚                    â€¢ Receives metrics/logs/traces                    â”‚  â”‚
â”‚  â”‚                    â€¢ Processes and forwards to backends              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                   â”‚
                                                                   â”‚ ALERTS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ALERTING LAYER                                       â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Alertmanager                                      â”‚   â”‚
â”‚  â”‚                    (Port 9093)                                       â”‚   â”‚
â”‚  â”‚                    â€¢ Receives alerts from Prometheus                 â”‚   â”‚
â”‚  â”‚                    â€¢ Routes to channels (email, Slack, PagerDuty)     â”‚   â”‚
â”‚  â”‚                    â€¢ Handles grouping, deduplication, silencing       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LOGS COLLECTION                                       â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Promtail                                           â”‚   â”‚
â”‚  â”‚                    (Port 9080)                                       â”‚   â”‚
â”‚  â”‚                    â€¢ Reads log files from filesystem                 â”‚   â”‚
â”‚  â”‚                    â€¢ Tracks position                                 â”‚   â”‚
â”‚  â”‚                    â€¢ Adds labels                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                                    â”‚
â”‚                          â”‚ PUSH (HTTP POST)                                   â”‚
â”‚                          â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Loki                                              â”‚   â”‚
â”‚  â”‚                    (Port 3100)                                       â”‚   â”‚
â”‚  â”‚                    â€¢ Receives logs via push API                     â”‚   â”‚
â”‚  â”‚                    â€¢ Stores in chunks                               â”‚   â”‚
â”‚  â”‚                    â€¢ Indexes for querying                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VISUALIZATION LAYER                                   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Grafana                                            â”‚   â”‚
â”‚  â”‚                    (Port 3000)                                       â”‚   â”‚
â”‚  â”‚                                                                       â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Data Sources:                                                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Prometheus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Loki â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”                              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Tempo (traces) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”                           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Alertmanager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”                        â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                 â”‚  â”‚  â”‚  â”‚                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â–¼â”€â”€â–¼â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Dashboards & Panels                                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Metrics visualization                                      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Log exploration                                            â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Alert visualization                                        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Trace visualization                                        â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
                            DATA FLOW SUMMARY
================================================================================

METRICS FLOW:
  Apps â†’ Push Gateway â†’ Prometheus â† Node Exporter
                              â†“
                        Alertmanager
                              â†“
                          Grafana

LOGS FLOW:
  Log Files â†’ Promtail â†’ Loki â†’ Grafana

TRACES FLOW (with Tempo):
  Apps â†’ Alloy â†’ Tempo â†’ Grafana

================================================================================
                            KEY INTERACTIONS
================================================================================

1. PROMETHEUS:
   - PULLS metrics from: Node Exporter, Push Gateway, Alloy, App endpoints
   - Uses Service Discovery to find targets
   - PUSHES alerts to: Alertmanager
   - Serves queries to: Grafana

2. PROMTAIL:
   - READS log files from filesystem
   - PUSHES logs to: Loki
   - Tracks position to avoid duplicates

3. LOKI:
   - RECEIVES logs via push API from Promtail
   - STORES logs in chunks
   - Serves queries to: Grafana

4. ALERTMANAGER:
   - RECEIVES alerts from: Prometheus
   - ROUTES alerts to: Email, Slack, PagerDuty, etc.

5. GRAFANA:
   - QUERIES metrics from: Prometheus
   - QUERIES logs from: Loki
   - QUERIES traces from: Tempo
   - DISPLAYS dashboards and visualizations

6. ALLOY:
   - RECEIVES telemetry from: Apps (OTLP)
   - PROCESSES and forwards to: Prometheus, Loki, Tempo

================================================================================


Promethus : Time series db for query, storing data
Amazon cloud watch : ?

Prometheus  web interface can be accessed using https only. Also grafana also checks for certificates

Exporter : Tool used by Prometheus to pull metrics from various services such as DBs , SQL, IOT device, HAProxy
            Eg Node exporters
Push Gateway : Tool used by backend apps to push metrics on to and then it will be pulled
                 by Prometheus in sometime

Node Exporter is an agent designed to collect hardware and operating system metrics from a server and
expose them in a format that Prometheus can understand. It's a key component in the Prometheus
monitoring ecosystem, acting as the bridge between your machine's low-level data and the Prometheus server.


How it Works âš™ï¸
The Prometheus server uses a pull-based model for monitoring, which means it actively requests metrics from its targets. Node Exporter serves as one of these targets. You install Node Exporter on each server you want to monitor. Once it's running, it exposes a /metrics endpoint, typically on port 9100.


When configured, the Prometheus server periodically scrapes this endpoint, collecting a wide range of metrics about the host machine.

Key Functions ğŸ“Š
Node Exporter provides a wealth of information about the host it's running on, including:

CPU usage: Tracks total CPU time, as well as time spent in various modes like user, system, and I/O wait.

Memory utilization: Provides details on available memory, free memory, and swap usage.

Disk I/O and space: Monitors disk reads and writes, as well as filesystem capacity and available space.

Network statistics: Collects metrics on network traffic, including bytes sent and received, packet errors, and connection states.

System metrics: Gathers information on system uptime, boot time, and process counts.

- Node Exporter can be extended with pluggable metrics collector




Install Node exporter (provided for linux env )
 wget https://github.com/prometheus/node_exporter/releases/download/v1.9.1/node_exporter-1.9.1.darwin-amd64.tar.gz

 Node exporter will run on 9100 port
 Health check link : http://localhost:9100/metrics


 Prometheus yml file is located in "/usr/local/etc"

 In Ubuntu, you create or run Node Exporter as a service . Coz if you run in terminal , it will die once you close the terminal


 Prmethus Model 
 - <Meteric Name> {key1=value1,key2=value2...}

 Prometheus Data types
  - Scalar : float ,string
  - Instant Vectors : An instant vector is a set of time series, each containing one sample (value) per series at a specific instant in time.
    Instant vectors are â€œsnapshotsâ€ of your system at a single moment in time (the query evaluation time).
    Eg up
    might return
    up{instance="web-1", job="api"}  1
    up{instance="web-2", job="api"}  0
    
        - common examples
            http_requests_total
            node_cpu_seconds_total
            memory_usage_bytes
  - A range vector is a set of time series where each series includes multiple data points over a time range.


  We have binary (< , == , >  etc) and arthematic operateor (+ ,- , /, * ,%,^)


Matcher and selectors

   1. A metric selector tells Prometheus which metric name and which series (labels) to fetch.

    Syntax:
    metric_name{label_matchers}
    
   2.  Matchers are conditions inside {} that filter which time series to include based on label names and values.

    Types of matchers:
        = exact match
        != not equal
        =~ regex match
        !~ regex not match

    Examples:
    node_cpu_seconds_total{mode="idle"}
    -> select CPU idle time across all nodes


Alerts in Prometheus
   they are usually kept at 10% of the point of chaos
   defined in Alerts Definition Yml file
   written in promql
   stored in prometheus server
   Need an alert manager to send notifications or emails in Promethus



Some Awesome Alert configs : https://samber.github.io/awesome-prometheus-alerts/rules#postgresql


Alert manager
 For mac system : we had to use Mac port to install alert managers. For some reason there wasn't a homebrew system is available


================================================================================
PROMETHEUS ALERTS, GROUPS, RULES & ALERTMANAGER - COMPREHENSIVE SUMMARY
================================================================================

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. ALERT GROUPS - WHAT THEY ARE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is a Group?
- A group is a logical container for related alert rules
- Helps organize alerts by category, priority, or service
- All rules in a group are evaluated together at the same time
- Each group can have its own evaluation interval (optional)

Structure:
  groups:
    - name: GroupName
      interval: 30s  # Optional: how often to evaluate this group
      rules:
        - alert: AlertName1
        - alert: AlertName2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. MULTIPLE GROUPS & MULTIPLE RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Can You Have Multiple Groups?
âœ“ YES - You can have multiple groups in the same file or across multiple files

Can You Have Multiple Rules in a Group?
âœ“ YES - A group can contain multiple rules

Example:
  groups:
    - name: InfrastructureAlerts
      rules:
        - alert: NodeExporterDown
        - alert: HighCPUUsage
        - alert: DiskFull
    
    - name: ApplicationAlerts
      rules:
        - alert: ServiceDown
        - alert: HighLatency

Best Practices:
- Organize by category (Infrastructure, Application, System Resources)
- Use different intervals for different priorities
- Group related alerts together for easier management

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. HOW RULES WORK - INDEPENDENT EVALUATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key Point: Rules are evaluated INDEPENDENTLY

What This Means:
- Each rule in a group is evaluated separately
- If one rule fires, it doesn't affect other rules
- Each rule creates its own alert when it fires
- Non-firing rules don't create alerts

Example Scenario:
  Group: SystemAlerts
    - NodeExporterDown: FIRES â†’ Creates alert â†’ Sent to Alertmanager
    - HighCPU: Doesn't fire â†’ No alert created
    - HighMemory: FIRES â†’ Creates alert â†’ Sent to Alertmanager

Result: Two separate alerts are sent to Alertmanager

Important:
- Rules in a group are only evaluated together (same time)
- But each rule operates independently
- Each firing rule creates its own alert
- Alertmanager receives each alert separately

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. ALERTMANAGER - GROUPING & NOTIFICATIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

How Alertmanager Groups Alerts:
- Alertmanager groups alerts by labels (like alertname, instance, severity)
- Default grouping: By 'alertname' only
- Grouping is controlled by 'group_by' setting in route configuration

Default Behavior:
  group_by: ['alertname']  # Default if not specified
  - All alerts with same alertname â†’ grouped together
  - Different alertnames â†’ separate groups

Custom Grouping Options:
  group_by: ['alertname', 'instance']  # Group by alertname AND instance
  group_by: ['severity']                # Group by severity only
  group_by: []                          # No grouping - one email per alert

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. EMAIL NOTIFICATIONS - HOW THEY WORK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

How Many Emails Are Sent?
- Alertmanager sends ONE EMAIL per alert group (not per alert)
- Multiple alerts in the same group â†’ ONE email with all alerts listed
- Different groups â†’ separate emails

Example: 3 identical alerts arrive
  - NodeExporterDown (server1)
  - NodeExporterDown (server2)
  - NodeExporterDown (server3)

Result: 1 EMAIL containing all 3 alerts separately

What's in the Email?
- Subject: Shows alert name and count [FIRING:3] NodeExporterDown
- Group labels: Common labels that define the group
- Individual alerts: Each alert listed separately with:
  * All its labels (instance, severity, job, etc.)
  * All its annotations (summary, description)
  * The metric value
  * Timestamp

Email Format Example:
  Subject: [FIRING:3] NodeExporterDown
  
  Alert Group: {alertname="NodeExporterDown", severity="critical"}
  
  Alerts (3):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Alert 1:
    Labels:
      - alertname: NodeExporterDown
      - instance: server1
      - severity: critical
    Annotations:
      - summary: Node Exporter is not running
      - description: Node Exporter is not running on server1
    Value: 0
  
  Alert 2:
    Labels:
      - alertname: NodeExporterDown
      - instance: server2
      ...
  
  Alert 3:
    Labels:
      - alertname: NodeExporterDown
      - instance: server3
      ...

Key Points:
âœ“ One email per group (not per alert)
âœ“ Each alert is listed separately with full details
âœ“ You can see which specific instances/servers are affected
âœ“ Subject shows count of alerts in the group

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. TIMING CONFIGURATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Alertmanager Route Timing Parameters:

group_wait: 10s
  - How long to wait before sending initial notification for a new group
  - If multiple alerts arrive within this time, they're grouped together
  
group_interval: 10s
  - How long to wait before sending a new notification about the same group
  - If new alerts join the group, wait this long before sending update
  
repeat_interval: 10m
  - How long to wait before repeating the same alert notification
  - Default is 4 hours if not specified
  - Controls how often you get reminded about the same alert

Example Timeline:
  T0:  3 alerts arrive â†’ Group starts
  T10s: group_wait expires â†’ First email sent (3 alerts)
  T20s: 1 new alert joins group â†’ group_interval starts
  T30s: group_interval expires â†’ Second email sent (4 alerts)
  T10m: repeat_interval expires â†’ Third email sent (still 4 alerts)

Best Practices:
- Critical alerts: Shorter intervals (group_wait: 5s, repeat_interval: 2m)
- Warning alerts: Longer intervals (group_wait: 30s, repeat_interval: 10m)
- Info alerts: Even longer intervals (group_wait: 1m, repeat_interval: 30m)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. PURPOSE OF MULTIPLE RULES IN A GROUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Why Have Multiple Rules in a Group?

1. Monitor Different Conditions
   - Each rule monitors different metrics or conditions
   - Example: ServiceDown, HighLatency, ErrorRateHigh, DiskFull

2. Different Severity Levels
   - Different rules can have different severity levels
   - Example: critical for ServiceDown, warning for HighCPU

3. Different Thresholds for Same Metric
   - Multiple rules for same metric with different thresholds
   - Example: CPUWarning (>70%), CPUCritical (>90%)

4. Different Time Windows
   - Different rules can have different 'for' durations
   - Example: ServiceDown (30s), ServiceDegraded (5m)

5. Different Routing
   - Different labels allow different routing in Alertmanager
   - Example: severity=critical â†’ urgent_receiver, severity=warning â†’ main_receiver

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. LABEL EVALUATION - WHEN RULES DON'T FIRE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

When Expression is FALSE:
- Alert is in "inactive" state
- Labels and annotations are NOT evaluated
- Template variables ({{ $labels.instance }}) are not populated
- This is by design - Prometheus doesn't waste resources

When Expression is TRUE:
- Alert is in "pending" or "firing" state
- Labels and annotations ARE evaluated
- Template variables are populated with actual values

Key Point: Labels and annotations are only evaluated when the alert is firing

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. COMPLETE WORKFLOW EXAMPLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: System with 3 alerts configured

Step 1: Prometheus Evaluates Rules
  - Group: SystemAlerts
    * NodeExporterDown: FIRES (up == 0 for 1m)
    * HighCPU: Doesn't fire (CPU is 60%)
    * HighMemory: FIRES (memory is 95%)

Step 2: Prometheus Creates Alerts
  - Alert 1: NodeExporterDown (severity: critical, instance: server1)
  - Alert 2: HighMemory (severity: warning, instance: server1)
  - No alert for HighCPU

Step 3: Alerts Sent to Alertmanager
  - NodeExporterDown â†’ Alertmanager
  - HighMemory â†’ Alertmanager

Step 4: Alertmanager Groups Alerts
  - Group 1: {alertname="NodeExporterDown", severity="critical"}
    â†’ Routes to urgent_receiver (matches severity=critical)
  - Group 2: {alertname="HighMemory", severity="warning"}
    â†’ Routes to main_receiver (default)

Step 5: Email Notifications Sent
  - Email 1: Critical alert for NodeExporterDown
  - Email 2: Warning alert for HighMemory

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Rules are evaluated independently - each rule operates separately
âœ“ Only firing rules create alerts - non-firing rules don't create alerts
âœ“ Each alert is sent individually to Alertmanager
âœ“ Alertmanager groups alerts by labels for notifications
âœ“ Multiple alerts in same group â†’ ONE email with all alerts listed separately
âœ“ Each alert in email shows full details (labels, annotations, values)
âœ“ Timing configuration controls how often notifications are sent
âœ“ Labels/annotations only evaluated when alert is firing
âœ“ Groups help organize related alerts together
âœ“ Multiple rules allow monitoring different conditions, severities, thresholds

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
INHIBITION RULES & SILENCING IN ALERTMANAGER - COMPREHENSIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. INHIBITION RULES - OVERVIEW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Inhibition?
- Inhibition rules suppress certain alerts when other alerts are firing
- Prevents alert flooding by hiding less important alerts when critical ones fire
- Example: If "ServerDown" is firing, suppress "HighCPU" alerts for that server
- Configured in Alertmanager configuration file (alertmanager.yml)

Key Concept:
  Source Alert (firing) â†’ Inhibits â†’ Target Alerts (suppressed)
  
  When source alert fires, target alerts matching the rule are suppressed
  Target alerts are not sent to receivers while source alert is active

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. INHIBITION RULE SYNTAX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Structure:
  inhibit_rules:
    - source_match:          # Source alert that triggers inhibition
        label_name: value
      target_match:          # Target alerts to be suppressed
        label_name: value
      equal: ['label1', 'label2']  # Labels that must match between source and target

Components:
  source_match: Conditions that identify the source alert (the inhibitor)
  target_match: Conditions that identify target alerts (to be suppressed)
  equal: Labels that must have the same value in both source and target

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. INHIBITION RULE EXAMPLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example 1: Suppress Warnings When Critical Alerts Fire
  inhibit_rules:
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'instance']
  
  Meaning:
    - When any critical alert fires
    - Suppress warning alerts with same alertname and instance
    - Prevents getting both critical and warning alerts for same issue

Example 2: Suppress All Alerts When Server is Down
  inhibit_rules:
    - source_match:
        alertname: ServerDown
      target_match:
        severity: warning
      equal: ['instance']
  
  Meaning:
    - When ServerDown alert fires for an instance
    - Suppress all warning alerts for that same instance
    - Makes sense: if server is down, no point alerting about high CPU

Example 3: Suppress Specific Alerts When Parent Alert Fires
  inhibit_rules:
    - source_match:
        alertname: ClusterDown
      target_match:
        alertname: NodeHighCPU
      equal: ['cluster']
  
  Meaning:
    - When ClusterDown fires
    - Suppress NodeHighCPU alerts in the same cluster
    - Cluster-level issue makes node-level alerts irrelevant

Example 4: Multiple Source Conditions
  inhibit_rules:
    - source_match:
        severity: critical
        component: database
      target_match:
        severity: warning
        component: database
      equal: ['instance']
  
  Meaning:
    - When critical database alert fires
    - Suppress warning database alerts for same instance

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. HOW INHIBITION WORKS - STEP BY STEP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Alerts Arrive at Alertmanager
  - Alert A: ServerDown (severity: critical, instance: server1)
  - Alert B: HighCPU (severity: warning, instance: server1)
  - Alert C: HighMemory (severity: warning, instance: server1)

Step 2: Inhibition Rules Evaluated
  Rule: source_match {severity: critical} â†’ target_match {severity: warning}
        equal: ['instance']
  
  Check:
    - Source alert (ServerDown) matches source_match? YES (severity: critical)
    - Target alerts (HighCPU, HighMemory) match target_match? YES (severity: warning)
    - Equal labels match? YES (instance: server1 for all)

Step 3: Target Alerts Suppressed
  - Alert A (ServerDown): NOT suppressed (it's the source)
  - Alert B (HighCPU): SUPPRESSED (matches target_match and equal)
  - Alert C (HighMemory): SUPPRESSED (matches target_match and equal)

Step 4: Only Source Alert Sent
  - Only ServerDown alert is sent to receivers
  - HighCPU and HighMemory are suppressed (not sent)

Step 5: When Source Alert Resolves
  - ServerDown alert resolves (server comes back up)
  - Inhibition stops
  - HighCPU and HighMemory alerts can now fire again if conditions still exist

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. SILENCING - OVERVIEW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Silencing?
- Silencing is a manual way to suppress alerts temporarily
- Done through Alertmanager UI or API
- Useful for planned maintenance, known issues, or testing
- More flexible than inhibition rules (can be created/removed on demand)

Key Differences from Inhibition:
  Inhibition Rules:
    - Automatic (configured in alertmanager.yml)
    - Based on alert conditions (source/target matching)
    - Always active when conditions are met
    - Requires configuration change to modify
  
  Silencing:
    - Manual (created through UI/API)
    - Based on matchers (label selectors)
    - Can have time limits (start/end time)
    - Can be created/removed without config changes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. HOW TO CREATE SILENCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Method 1: Through Alertmanager UI
  1. Go to http://localhost:9093
  2. Click on "Silences" tab
  3. Click "New Silence" button
  4. Fill in:
     - Matchers: Label selectors (e.g., alertname="HighCPU", instance="server1")
     - Starts at: When silence should start
     - Ends at: When silence should end (or duration)
     - Created by: Your name/email
     - Comment: Reason for silence (e.g., "Planned maintenance")
  5. Click "Create"

Method 2: Through API
  POST http://localhost:9093/api/v2/silences
  
  Body:
  {
    "matchers": [
      {"name": "alertname", "value": "HighCPU", "isRegex": false},
      {"name": "instance", "value": "server1", "isRegex": false}
    ],
    "startsAt": "2025-11-08T10:00:00Z",
    "endsAt": "2025-11-08T12:00:00Z",
    "createdBy": "admin@example.com",
    "comment": "Planned maintenance window"
  }

Method 3: Using amtool (Command Line)
  amtool silence add \
    alertname=HighCPU instance=server1 \
    --start=2025-11-08T10:00:00Z \
    --end=2025-11-08T12:00:00Z \
    --comment="Planned maintenance"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. SILENCE MATCHERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Matchers are label selectors that determine which alerts to silence.

Types of Matchers:
  1. Exact Match:
     alertname="HighCPU"
     instance="server1"
  
  2. Regex Match:
     alertname=~"High.*"
     instance=~"server[0-9]+"
  
  3. Negative Match:
     alertname!="HighCPU"
     severity!="info"

Examples:
  Silence all HighCPU alerts:
    alertname="HighCPU"
  
  Silence all alerts on server1:
    instance="server1"
  
  Silence all critical alerts:
    severity="critical"
  
  Silence HighCPU on specific server:
    alertname="HighCPU"
    instance="server1"
  
  Silence all alerts matching pattern:
    alertname=~"High.*"
    instance=~"server[0-9]+"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. SILENCE DURATION & EXPIRATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Time-Based Silencing:
  - Silences can have start and end times
  - Useful for planned maintenance windows
  - Automatically expires when end time is reached

Example:
  Starts at: 2025-11-08 10:00:00
  Ends at:   2025-11-08 12:00:00
  Duration:  2 hours
  
  - Alerts matching the silence are suppressed from 10:00 to 12:00
  - After 12:00, silence expires and alerts can fire again

Permanent Silences:
  - Can set end time far in the future
  - Or use very long duration
  - Remember to manually expire when no longer needed

Expiring Silences:
  - Silences automatically expire at their end time
  - Can also be manually expired through UI/API
  - Expired silences are kept in history for reference

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. INHIBITION VS SILENCING - COMPARISON
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature             â”‚ Inhibition Rules      â”‚ Silencing            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Configuration       â”‚ alertmanager.yml      â”‚ UI/API/amtool        â”‚
â”‚ Type                â”‚ Automatic             â”‚ Manual               â”‚
â”‚ Trigger             â”‚ Based on alert state  â”‚ Based on matchers    â”‚
â”‚ Duration            â”‚ While source fires    â”‚ Time-based or manual â”‚
â”‚ Flexibility         â”‚ Fixed rules           â”‚ Very flexible        â”‚
â”‚ Use Case            â”‚ Alert relationships   â”‚ Maintenance/testing  â”‚
â”‚ Modification         â”‚ Config change + reload  â”‚ Instant via UI      â”‚
â”‚ Scope               â”‚ Sourceâ†’Target logic    â”‚ Any matcher pattern  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When to Use Inhibition:
  âœ“ Automatic suppression based on alert relationships
  âœ“ Reduce alert noise when critical issues occur
  âœ“ Permanent rules for known alert dependencies
  âœ“ Example: Suppress warnings when critical alerts fire

When to Use Silencing:
  âœ“ Planned maintenance windows
  âœ“ Known issues being worked on
  âœ“ Testing alert configurations
  âœ“ Temporary suppression of noisy alerts
  âœ“ One-off situations

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. COMPLETE EXAMPLE - INHIBITION RULE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Configuration:
  inhibit_rules:
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'instance', 'job']

Scenario:
  Alerts Firing:
    1. ServerDown (severity: critical, instance: server1, job: web)
    2. HighCPU (severity: warning, instance: server1, job: web)
    3. HighMemory (severity: warning, instance: server1, job: web)
    4. HighCPU (severity: warning, instance: server2, job: web)

Evaluation:
  Source Match: ServerDown (severity: critical) âœ“
  
  Target Match Check:
    - HighCPU on server1: severity=warning âœ“, equal labels match âœ“ â†’ SUPPRESSED
    - HighMemory on server1: severity=warning âœ“, equal labels match âœ“ â†’ SUPPRESSED
    - HighCPU on server2: severity=warning âœ“, equal labels DON'T match âœ— â†’ NOT SUPPRESSED

Result:
  - ServerDown: Sent (source alert)
  - HighCPU on server1: Suppressed (matches target and equal)
  - HighMemory on server1: Suppressed (matches target and equal)
  - HighCPU on server2: Sent (equal labels don't match - different instance)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
11. COMPLETE EXAMPLE - SILENCING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: Planned maintenance on server1 from 10:00 to 12:00

Step 1: Create Silence
  Matchers:
    - instance="server1"
  
  Time:
    - Starts: 2025-11-08 10:00:00
    - Ends:   2025-11-08 12:00:00
  
  Comment: "Planned maintenance - server1"

Step 2: During Maintenance
  Alerts that would fire:
    - ServerDown (instance: server1) â†’ SUPPRESSED by silence
    - HighCPU (instance: server1) â†’ SUPPRESSED by silence
    - HighMemory (instance: server1) â†’ SUPPRESSED by silence
    - HighCPU (instance: server2) â†’ NOT suppressed (different instance)

Step 3: After Maintenance
  - Silence expires at 12:00
  - Alerts for server1 can fire again normally

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
12. BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Rules:
  âœ“ Keep rules simple and clear
  âœ“ Document why each rule exists
  âœ“ Test rules carefully before deploying
  âœ“ Use 'equal' labels to ensure proper matching
  âœ“ Don't over-inhibit - you might miss important alerts

Silencing:
  âœ“ Always add comments explaining why silence was created
  âœ“ Set appropriate end times (don't leave permanent silences)
  âœ“ Review and expire old silences regularly
  âœ“ Use specific matchers (don't silence too broadly)
  âœ“ Document maintenance windows in silence comments

General:
  âœ“ Use inhibition for automatic, permanent relationships
  âœ“ Use silencing for temporary, manual situations
  âœ“ Monitor silenced alerts to ensure they're appropriate
  âœ“ Review silence history periodically
  âœ“ Combine both: inhibition for logic, silencing for exceptions

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
13. COMMON USE CASES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Use Cases:
  1. Suppress warnings when critical alerts fire
  2. Suppress node-level alerts when cluster-level alert fires
  3. Suppress application alerts when infrastructure alert fires
  4. Suppress dependent alerts when root cause alert fires

Silencing Use Cases:
  1. Planned maintenance windows
  2. Known issues being actively worked on
  3. Testing alert configurations
  4. Suppressing noisy alerts temporarily
  5. Scheduled downtime periods
  6. Development/testing environments

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
14. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Inhibition Rules:
  âœ“ Automatic suppression based on alert relationships
  âœ“ Configured in alertmanager.yml
  âœ“ Source alert suppresses target alerts
  âœ“ Uses source_match, target_match, and equal labels
  âœ“ Active while source alert is firing

Silencing:
  âœ“ Manual suppression via UI/API/amtool
  âœ“ Based on label matchers
  âœ“ Can be time-limited
  âœ“ More flexible than inhibition
  âœ“ Useful for temporary situations

Both:
  âœ“ Help reduce alert noise
  âœ“ Prevent alert flooding
  âœ“ Improve signal-to-noise ratio
  âœ“ Should be used thoughtfully
  âœ“ Need proper documentation

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
RECORDING RULES IN PROMETHEUS - COMPREHENSIVE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. WHAT ARE RECORDING RULES?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition:
- Recording rules pre-compute frequently used or expensive queries
- They store the result as a new time series with a new metric name
- Results are stored in Prometheus's time series database
- Can be queried like any other metric

Purpose:
- Improve query performance (pre-compute expensive calculations)
- Simplify complex queries (create reusable metrics)
- Reduce query load on Prometheus
- Create aggregated metrics for dashboards
- Standardize metric names across environments

Key Difference from Alert Rules:
  Alert Rules:
    - Evaluate conditions and create alerts
    - Don't store new metrics
    - Used for alerting only
  
  Recording Rules:
    - Evaluate expressions and store results as new metrics
    - Create new time series data
    - Used for querying and dashboards

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. RECORDING RULE SYNTAX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Structure:
  groups:
    - name: group_name
      interval: 30s  # Optional: evaluation interval
      rules:
        - record: new_metric_name
          expr: promql_expression

Components:
  record: The name of the new metric to create
  expr: The PromQL expression to evaluate
  labels: Optional labels to add to the new metric

Example:
  groups:
    - name: cpu_recording_rules
      interval: 30s
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. WHY USE RECORDING RULES?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Performance Optimization
   - Pre-compute expensive queries (aggregations, rate calculations)
   - Reduce query time for dashboards
   - Lower CPU usage on Prometheus server
   - Faster response times for frequently accessed metrics

2. Query Simplification
   - Complex expressions become simple metric queries
   - Example: Instead of:
       rate(http_requests_total[5m]) / rate(http_requests_total[5m]) * 100
     Use pre-computed: http_request_error_rate

3. Reusability
   - Define once, use everywhere
   - Consistent calculations across dashboards
   - Easier to maintain

4. Aggregation
   - Create cluster-level, service-level, or environment-level metrics
   - Aggregate across multiple instances
   - Create summary metrics

5. Standardization
   - Consistent metric naming conventions
   - Standardize metric names across different exporters
   - Create unified metrics from different sources

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. RECORDING RULE EXAMPLES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Example 1: CPU Usage Percentage
  groups:
    - name: cpu_recording_rules
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: |
            100 - (
              avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) 
              * 100
            )
  
  Usage: Query `instance:node_cpu_usage:rate5m` instead of complex expression

Example 2: Memory Usage Percentage
  groups:
    - name: memory_recording_rules
      rules:
        - record: instance:node_memory_usage:ratio
          expr: |
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
            / node_memory_MemTotal_bytes
  
  Usage: Query `instance:node_memory_usage:ratio` for memory usage

Example 3: HTTP Request Rate
  groups:
    - name: http_recording_rules
      rules:
        - record: http:requests:rate5m
          expr: rate(http_requests_total[5m])
  
  Usage: Query `http:requests:rate5m` instead of `rate(http_requests_total[5m])`

Example 4: Error Rate Percentage
  groups:
    - name: http_recording_rules
      rules:
        - record: http:request_error_rate:ratio
          expr: |
            rate(http_requests_total{status=~"5.."}[5m]) 
            / rate(http_requests_total[5m])
  
  Usage: Query `http:request_error_rate:ratio` for error percentage

Example 5: Aggregated Metrics
  groups:
    - name: cluster_recording_rules
      rules:
        - record: cluster:cpu_usage:avg
          expr: avg(instance:node_cpu_usage:rate5m)
          labels:
            cluster: production
        
        - record: cluster:memory_usage:avg
          expr: avg(instance:node_memory_usage:ratio)
          labels:
            cluster: production

Example 6: Multi-step Calculation
  groups:
    - name: service_recording_rules
      rules:
        # Step 1: Calculate request rate
        - record: service:http_requests:rate5m
          expr: rate(http_requests_total[5m])
        
        # Step 2: Calculate error rate using step 1
        - record: service:http_errors:rate5m
          expr: rate(http_requests_total{status=~"5.."}[5m])
        
        # Step 3: Calculate error percentage using steps 1 and 2
        - record: service:http_error_rate:ratio
          expr: |
            service:http_errors:rate5m 
            / service:http_requests:rate5m

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. NAMING CONVENTIONS FOR RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Best Practice Naming Pattern:
  level:metric:operation

Components:
  level: The aggregation level (instance, service, cluster, etc.)
  metric: The metric name being recorded
  operation: The operation performed (rate, sum, avg, ratio, etc.)

Examples:
  instance:node_cpu_usage:rate5m
    - level: instance (per-instance metric)
    - metric: node_cpu_usage
    - operation: rate5m (5-minute rate)
  
  cluster:http_requests:sum
    - level: cluster (cluster-wide)
    - metric: http_requests
    - operation: sum (summed across instances)
  
  service:memory_usage:avg
    - level: service (service-level)
    - metric: memory_usage
    - operation: avg (averaged)

Common Operations:
  :rate5m    - 5-minute rate
  :rate1m    - 1-minute rate
  :sum       - Summed value
  :avg       - Average value
  :max       - Maximum value
  :min       - Minimum value
  :ratio     - Ratio/percentage
  :increase  - Increase over time

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. HOW TO SAVE RECORDING RULES - FILE ORGANIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Best Practice: Separate Files by Purpose

Option 1: Separate Files (RECOMMENDED)
  Directory Structure:
    /usr/local/etc/rule/
      â”œâ”€â”€ alerts.yml          # Alert rules only
      â”œâ”€â”€ recording_cpu.yml    # CPU-related recording rules
      â”œâ”€â”€ recording_memory.yml # Memory-related recording rules
      â”œâ”€â”€ recording_http.yml  # HTTP-related recording rules
      â””â”€â”€ recording_cluster.yml # Cluster-level recording rules

  prometheus.yml:
    rule_files:
      - "/usr/local/etc/rule/alerts.yml"
      - "/usr/local/etc/rule/recording_cpu.yml"
      - "/usr/local/etc/rule/recording_memory.yml"
      - "/usr/local/etc/rule/recording_http.yml"
      - "/usr/local/etc/rule/recording_cluster.yml"

Option 2: Single File with Groups
  File: /usr/local/etc/rule/recording_rules.yml
  
  groups:
    - name: cpu_recording_rules
      rules: [...]
    - name: memory_recording_rules
      rules: [...]
    - name: http_recording_rules
      rules: [...]

  prometheus.yml:
    rule_files:
      - "/usr/local/etc/rule/alerts.yml"
      - "/usr/local/etc/rule/recording_rules.yml"

Option 3: By Service/Component
  Directory Structure:
    /usr/local/etc/rule/
      â”œâ”€â”€ alerts.yml
      â”œâ”€â”€ node_exporter_recording.yml
      â”œâ”€â”€ application_recording.yml
      â””â”€â”€ infrastructure_recording.yml

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. CONFIGURING RECORDING RULES IN PROMETHEUS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Create Recording Rules File
  Create file: /usr/local/etc/rule/recording_rules.yml
  
  groups:
    - name: cpu_recording_rules
      interval: 30s
      rules:
        - record: instance:node_cpu_usage:rate5m
          expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

Step 2: Add to prometheus.yml
  rule_files:
    - "/usr/local/etc/rule/alerts.yml"
    - "/usr/local/etc/rule/recording_rules.yml"

Step 3: Reload Prometheus
  curl -X POST http://localhost:9090/-/reload
  
  Or restart Prometheus service

Step 4: Verify Rules Are Loaded
  - Go to http://localhost:9090/rules
  - Check that recording rules appear
  - Verify they're being evaluated

Step 5: Query the New Metric
  - In Prometheus UI, query: instance:node_cpu_usage:rate5m
  - Should return time series data

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. BEST PRACTICES FOR SAVING RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. File Organization
   âœ“ Separate recording rules from alert rules
   âœ“ Group related rules together
   âœ“ Use descriptive file names
   âœ“ Keep files focused and manageable size
   âœ“ Document what each file contains

2. Naming Conventions
   âœ“ Use consistent naming pattern (level:metric:operation)
   âœ“ Make names descriptive and self-documenting
   âœ“ Avoid generic names like "metric1", "metric2"
   âœ“ Include operation type in name (rate, sum, avg)
   âœ“ Use underscores, not hyphens or spaces

3. Group Organization
   âœ“ Group by metric type (CPU, memory, network)
   âœ“ Group by service/component
   âœ“ Group by aggregation level (instance, cluster)
   âœ“ Use descriptive group names
   âœ“ Keep groups focused on single purpose

4. Evaluation Intervals
   âœ“ Set appropriate intervals based on use case
   âœ“ Shorter intervals for frequently queried metrics
   âœ“ Longer intervals for less critical metrics
   âœ“ Consider query frequency when setting intervals
   âœ“ Default: Uses global evaluation_interval if not specified

5. Documentation
   âœ“ Add comments explaining complex expressions
   âœ“ Document why recording rule exists
   âœ“ Note dependencies between rules
   âœ“ Include examples of how to use the metric
   âœ“ Document any assumptions or limitations

6. Version Control
   âœ“ Store rules in version control (git)
   âœ“ Use same workflow as alert rules
   âœ“ Review changes before deploying
   âœ“ Test rules in development first
   âœ“ Keep history of changes

7. Testing
   âœ“ Test expressions in Prometheus UI first
   âœ“ Verify results match expectations
   âœ“ Check for errors in Prometheus logs
   âœ“ Monitor rule evaluation performance
   âœ“ Validate metric names are correct

8. Maintenance
   âœ“ Review unused recording rules periodically
   âœ“ Remove obsolete rules
   âœ“ Update rules when source metrics change
   âœ“ Monitor rule evaluation time
   âœ“ Optimize expensive rules

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. COMPLETE EXAMPLE - RECORDING RULES FILE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

File: /usr/local/etc/rule/recording_rules.yml

groups:
  # CPU-related recording rules
  - name: cpu_recording_rules
    interval: 30s
    rules:
      # CPU usage percentage (5-minute rate)
      - record: instance:node_cpu_usage:rate5m
        expr: |
          100 - (
            avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) 
            * 100
          )
      
      # CPU usage by mode
      - record: instance:node_cpu_seconds:rate5m
        expr: rate(node_cpu_seconds_total[5m])
  
  # Memory-related recording rules
  - name: memory_recording_rules
    interval: 30s
    rules:
      # Memory usage ratio
      - record: instance:node_memory_usage:ratio
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
          / node_memory_MemTotal_bytes
      
      # Memory usage percentage
      - record: instance:node_memory_usage:percent
        expr: |
          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) 
          / node_memory_MemTotal_bytes 
          * 100
  
  # Disk-related recording rules
  - name: disk_recording_rules
    interval: 1m
    rules:
      # Disk usage ratio
      - record: instance:node_filesystem_usage:ratio
        expr: |
          (node_filesystem_size_bytes - node_filesystem_avail_bytes) 
          / node_filesystem_size_bytes
      
      # Disk usage percentage
      - record: instance:node_filesystem_usage:percent
        expr: |
          (node_filesystem_size_bytes - node_filesystem_avail_bytes) 
          / node_filesystem_size_bytes 
          * 100

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. INTEGRATION WITH YOUR CURRENT SETUP
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Current Setup:
  - Config files in: /usr/local/etc/
  - Alert rules in: /usr/local/etc/rule/alerts.yml
  - Repo tracking: config/rule/alerts.yml
  - Sync script: sync-config.sh

Recommended Addition:
  1. Create recording rules file:
     config/rule/recording_rules.yml
  
  2. Update prometheus.yml:
     rule_files:
       - "/usr/local/etc/rule/alerts.yml"
       - "/usr/local/etc/rule/recording_rules.yml"
  
  3. Update sync-config.sh to include recording rules:
     sudo cp config/rule/recording_rules.yml /usr/local/etc/rule/recording_rules.yml
  
  4. Sync and reload:
     ./sync-config.sh
     curl -X POST http://localhost:9090/-/reload

Directory Structure:
  config/
    â”œâ”€â”€ prometheus.yml
    â””â”€â”€ rule/
        â”œâ”€â”€ alerts.yml
        â””â”€â”€ recording_rules.yml

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
11. COMMON PATTERNS AND USE CASES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Pattern 1: Rate Calculations
  - Pre-compute rate() for frequently queried counters
  - Reduces query time for dashboards
  - Example: HTTP request rates, error rates

Pattern 2: Aggregations
  - Create cluster/service-level metrics
  - Aggregate across multiple instances
  - Example: Average CPU across cluster

Pattern 3: Percentages and Ratios
  - Convert raw values to percentages
  - Calculate ratios between metrics
  - Example: CPU usage %, error rate %

Pattern 4: Multi-step Calculations
  - Break complex calculations into steps
  - Each step creates a reusable metric
  - Example: Error rate = errors / total requests

Pattern 5: Standardization
  - Normalize metric names from different exporters
  - Create unified naming conventions
  - Example: Standardize CPU metric names

Pattern 6: Historical Aggregations
  - Pre-compute aggregations over time windows
  - Useful for long-term trends
  - Example: Daily/weekly averages

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
12. MONITORING RECORDING RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Check Rule Status:
  - Prometheus UI: http://localhost:9090/rules
  - Shows all recording rules and their status
  - Indicates if rules are being evaluated

Check Rule Performance:
  - Prometheus UI: http://localhost:9090/rules
  - Shows evaluation time for each rule
  - Monitor for slow rules

Check Metrics Exist:
  - Query the recorded metric in Prometheus UI
  - Verify data is being stored
  - Check metric labels are correct

Common Issues:
  - Rules not loading: Check file path in prometheus.yml
  - No data: Check source metrics exist
  - Errors: Check Prometheus logs
  - Slow evaluation: Optimize expressions or increase interval

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
13. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Recording rules pre-compute and store query results as new metrics
âœ“ Use them to improve performance and simplify queries
âœ“ Follow naming convention: level:metric:operation
âœ“ Separate recording rules from alert rules in different files
âœ“ Organize rules by purpose (CPU, memory, HTTP, etc.)
âœ“ Set appropriate evaluation intervals
âœ“ Document complex expressions
âœ“ Test rules before deploying
âœ“ Monitor rule performance
âœ“ Keep rules in version control
âœ“ Use consistent file organization
âœ“ Group related rules together

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PUSHGATEWAY - SUMMARY AND PROMETHEUS CONFIG CHANGES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Pushgateway?
- A service that accepts metrics pushed by shortâ€‘lived/batch jobs and exposes them to Prometheus via /metrics (default port 9091).
- Not a general-purpose ingestion layer; avoid using it for longâ€‘running services that can be scraped directly.

When to use
- Cron jobs, CI/CD steps, and ephemeral batch jobs that exit before Prometheus can scrape them.

Prometheus scrape config (preserve pushed labels):

  scrape_configs:
    - job_name: "pushgateway"
      honor_labels: true        # keep job/instance from pushed metrics
      static_configs:
        - targets: ["localhost:9091"]

Why honor_labels: true?
- Pushgateway encodes the grouping key (e.g., job, instance) as labels.
- Without honor_labels, the scraperâ€™s labels (job="pushgateway", instance="host:9091") would override the pushed labels.

Pushing metrics (HTTP API)
- Create/replace:   PUT  /metrics/job/<job_name>[/instance/<instance>]
- Add/merge:        POST /metrics/job/<job_name>[/instance/<instance>]
- Delete all for job:       DELETE /metrics/job/<job_name>
- Delete one instance:      DELETE /metrics/job/<job_name>/instance/<instance>

Examples (text exposition format):

  # push for a job and instance
  curl -X PUT \
    --data-binary "# HELP my_job_last_success Unix time of last success
# TYPE my_job_last_success gauge
my_job_last_success $(date +%s)" \
    http://localhost:9091/metrics/job/my_batch/instance/host1

  # delete metrics for that instance
  curl -X DELETE http://localhost:9091/metrics/job/my_batch/instance/host1

Best practices
- Use distinct job and instance values in the push URL; avoid embedding hostnames in metric names.
- Keep payloads small and metric/label names stable and consistent.
- Delete metrics on successful completion to avoid stale data.
- Do not expose Pushgateway publicly; protect with network controls or an auth proxy.
- Consider recording rules on top of pushed metrics for dashboards.

Running the binary
- From project root: ./pushgateway/pushgateway  (listens on port 9091)
- Check version/type: ./pushgateway/pushgateway --version
- If not executable: chmod +x pushgateway/pushgateway
==============================================

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COLLECTORREGISTRY IN PROMETHEUS CLIENT - SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. WHAT IS COLLECTORREGISTRY?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Definition:
- CollectorRegistry is a component from Prometheus client libraries that manages
  metric collectors and formats them for Prometheus consumption
- Acts as a container/organizer for metrics in your application
- Client-side component (not part of Prometheus server)

Purpose:
- Tracks and manages all metric collectors in your application
- Formats metrics in Prometheus exposition format
- Provides metrics when /metrics endpoint is scraped
- Organizes metrics for exposure to Prometheus

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. WHEN IS REGISTRY CREATED?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

In Pushgateway:
- Registry is created ONCE when Pushgateway starts up
- Single registry instance persists for entire process lifetime
- All push requests use the same registry
- Metrics accumulate in registry until deleted or restart
- On restart: New empty registry created (previous metrics lost unless persisted)

In Client Applications:
- Default Registry: Created automatically by Prometheus client library
  - Global singleton instance
  - All metrics automatically registered to it
  - Used when exposing /metrics endpoint
  
- Custom Registry: Created explicitly by developer
  - Created when you call: CollectorRegistry()
  - Isolated from default registry
  - Useful for pushing to Pushgateway, testing, or multiple metric sets

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. WHAT HAPPENS WHEN YOU CALL COLLECTORREGISTRY METHODS?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Creating a Registry:
  registry = CollectorRegistry()
  â†’ Creates empty registry instance
  â†’ Initializes internal data structures
  â†’ Ready to accept metric collectors
  â†’ No metrics registered yet

Registering a Metric:
  counter = Counter('http_requests_total', registry=registry)
  â†’ Metric added to registry's internal collection
  â†’ Registry tracks metric's name, type, labels, current value
  â†’ Registry maintains reference to collector
  â†’ Metric will appear when /metrics is scraped

Registering Multiple Metrics:
  counter = Counter('requests', registry=registry)
  gauge = Gauge('connections', registry=registry)
  â†’ Each metric stored separately in registry
  â†’ Registry maintains list/map of all registered collectors
  â†’ All collectors tracked together
  â†’ All metrics formatted together when exposed

Collecting Metrics (formatting):
  output = generate_latest(registry)
  â†’ Registry iterates through all registered collectors
  â†’ Calls collect() on each collector to get current values
  â†’ Formats each metric in Prometheus exposition format
  â†’ Returns string with all metrics formatted for Prometheus

Unregistering a Collector:
  registry.unregister(counter)
  â†’ Removes collector from registry's internal collection
  â†’ Metric no longer tracked
  â†’ Won't appear in future /metrics output
  â†’ Memory freed (if no other references)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. KEY METHODS AND THEIR EFFECTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

register(collector):
  - Adds collector to registry
  - Registry starts tracking this collector
  - Collector will appear in /metrics output

unregister(collector):
  - Removes collector from registry
  - Collector no longer tracked
  - Won't appear in future /metrics output

collect() (internal):
  - Iterates through all registered collectors
  - Calls each collector's collect() method
  - Gathers current metric values
  - Returns formatted metric data

get_sample_value(metric_name):
  - Looks up specific metric in registry
  - Returns current value of that metric
  - Useful for programmatic access

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. DEFAULT REGISTRY VS CUSTOM REGISTRY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Default Registry (Automatic):
  from prometheus_client import Counter
  
  counter = Counter('http_requests_total')
  # Uses default registry automatically
  
  â†’ Prometheus client maintains global default registry
  â†’ Metrics automatically registered to default registry
  â†’ When you expose /metrics, it uses default registry
  â†’ All metrics share same default registry

Custom Registry (Explicit):
  from prometheus_client import CollectorRegistry, Counter
  
  registry = CollectorRegistry()
  counter = Counter('http_requests_total', registry=registry)
  
  â†’ Creates separate, isolated registry
  â†’ Metrics only in this custom registry
  â†’ Useful for:
    * Pushing to Pushgateway (isolated metrics)
    * Multiple metric sets
    * Testing (separate test metrics)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. COMPLETE FLOW EXAMPLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Step 1: Create registry
  registry = CollectorRegistry()
  â†’ Empty registry created

Step 2: Create and register metrics
  counter = Counter('requests_total', registry=registry)
  gauge = Gauge('active_connections', registry=registry)
  â†’ Metrics added to registry's internal collection

Step 3: Update metrics
  counter.inc()  # Increment counter
  gauge.set(10)  # Set gauge value
  â†’ Metric values updated, registry still tracks them

Step 4: Collect and format (when /metrics is scraped)
  output = generate_latest(registry)
  â†’ Registry:
    1. Iterates through all registered collectors
    2. Calls collect() on each to get current values
    3. Formats in Prometheus exposition format
    4. Returns formatted string

Output:
  # HELP requests_total ...
  # TYPE requests_total counter
  requests_total 1.0
  # HELP active_connections ...
  # TYPE active_connections gauge
  active_connections 10.0

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. REGISTRY STATE MANAGEMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Registry Maintains:
  âœ“ List of all registered collectors
  âœ“ Metric metadata (names, types, help text)
  âœ“ Current metric values (via collectors)
  âœ“ Label combinations for each metric

Registry Does NOT:
  âœ— Store historical values (Prometheus does that)
  âœ— Persist data to disk (in-memory only)
  âœ— Send metrics to Prometheus (Prometheus pulls from /metrics)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ CollectorRegistry is a client-side metric manager
âœ“ Created once at startup (Pushgateway) or when needed (client apps)
âœ“ Tracks all registered metric collectors
âœ“ Formats metrics for Prometheus exposition format
âœ“ Provides metrics when /metrics endpoint is scraped
âœ“ Default registry is automatic, custom registry is explicit
âœ“ Registry maintains current state, not historical data
âœ“ Metrics persist in registry until unregistered or app restarts
âœ“ In Pushgateway: Single registry for entire process lifetime
âœ“ In Client Apps: Can have multiple registries for different purposes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PROMETHEUS AUTHENTICATION METHODS - SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. OVERVIEW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Important Note:
- Prometheus does NOT have built-in authentication for UI/API
- By default, Prometheus UI and API are unauthenticated
- Authentication must be added via external methods or web.config.file
- Scrape target authentication IS built-in
- Alertmanager authentication IS built-in
- Remote write authentication IS built-in

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. UI/API AUTHENTICATION METHODS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Method 1: Reverse Proxy with Authentication
  - Use reverse proxy (nginx, Apache, Traefik) in front of Prometheus
  - Proxy handles authentication, Prometheus runs behind it
  - Supports: Basic Auth, OAuth2, LDAP, SAML, JWT, API keys
  
  Example (Nginx Basic Auth):
    server {
      listen 9090;
      location / {
        auth_basic "Prometheus";
        auth_basic_user_file /etc/nginx/.htpasswd;
        proxy_pass http://localhost:9090;
      }
    }

Method 2: Web Configuration File (Prometheus 2.x+)
  - Configure TLS and basic authentication via web.config.file
  - Supports TLS, basic auth, and headers
  
  web-config.yml:
    tls_server_config:
      cert_file: /path/to/cert.pem
      key_file: /path/to/key.pem
    
    basic_auth_users:
      admin: $2y$10$hashed_password
      user: $2y$10$hashed_password
  
  Start Prometheus:
    prometheus --web.config.file=web-config.yml
  
  Generate password hash:
    htpasswd -nBC 10 admin

Method 3: TLS/SSL (HTTPS)
  - Encrypts traffic between client and Prometheus
  - Does NOT authenticate users, but secures communication
  - Requires certificates
  
  web-config.yml:
    tls_server_config:
      cert_file: /path/to/cert.pem
      key_file: /path/to/key.pem

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. SCRAPE TARGET AUTHENTICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Authentication:
  scrape_configs:
    - job_name: 'secure-service'
      basic_auth:
        username: 'scraper'
        password: 'secret'
      static_configs:
        - targets: ['service:8080']

Bearer Token:
  scrape_configs:
    - job_name: 'api-service'
      bearer_token: 'your-bearer-token-here'
      static_configs:
        - targets: ['api:8080']

Bearer Token File:
  scrape_configs:
    - job_name: 'api-service'
      bearer_token_file: '/path/to/token'
      static_configs:
        - targets: ['api:8080']

TLS Configuration:
  scrape_configs:
    - job_name: 'secure-service'
      scheme: https
      tls_config:
        ca_file: /path/to/ca.crt
        cert_file: /path/to/client.crt
        key_file: /path/to/client.key
        insecure_skip_verify: false
      static_configs:
        - targets: ['service:8443']

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. ALERTMANAGER AUTHENTICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Authentication:
  alerting:
    alertmanagers:
      - basic_auth:
          username: 'prometheus'
          password: 'secret'
        static_configs:
          - targets:
            - 'alertmanager:9093'

Bearer Token:
  alerting:
    alertmanagers:
      - bearer_token: 'alertmanager-token'
        static_configs:
          - targets:
            - 'alertmanager:9093'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. REMOTE WRITE AUTHENTICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Basic Authentication:
  remote_write:
    - url: 'https://remote-storage/api/v1/write'
      basic_auth:
        username: 'prometheus'
        password: 'secret'

Bearer Token:
  remote_write:
    - url: 'https://remote-storage/api/v1/write'
      bearer_token: 'remote-token'

OAuth2:
  remote_write:
    - url: 'https://remote-storage/api/v1/write'
      oauth2:
        client_id: 'prometheus'
        client_secret: 'secret'
        token_url: 'https://auth-server/oauth/token'

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. NETWORK-LEVEL SECURITY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Methods:
  - Firewall rules (restrict access to specific IPs)
  - VPN access (require VPN connection)
  - Private networks (deploy in private network)
  - IP whitelisting (via reverse proxy)

Use Cases:
  - Additional layer of security
  - Network isolation
  - Defense in depth

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. KUBERNETES AUTHENTICATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Methods:
  - Service accounts and RBAC (Kubernetes native auth)
  - Network policies (restrict pod-to-pod communication)
  - Ingress with authentication (Ingress controller with auth)
  - mTLS between pods (service mesh)

Service Mesh (Istio, Linkerd):
  - Automatic mTLS between services
  - Works well in Kubernetes
  - Provides encryption and authentication

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. AUTHENTICATION METHODS SUMMARY TABLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method               â”‚ Use Case         â”‚ Where Applied     â”‚ Built-in    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Reverse Proxy        â”‚ UI/API access    â”‚ Front of Prometheusâ”‚ No (external)â”‚
â”‚ Web Config File      â”‚ UI/API access    â”‚ Prometheus config â”‚ Yes (2.x+)  â”‚
â”‚ TLS/SSL              â”‚ Encryption       â”‚ Prometheus server â”‚ Yes         â”‚
â”‚ Basic Auth (scrape)  â”‚ Target scraping  â”‚ scrape_configs    â”‚ Yes         â”‚
â”‚ Bearer Token (scrape)â”‚ Target scraping  â”‚ scrape_configs    â”‚ Yes         â”‚
â”‚ TLS (scrape)         â”‚ Secure targets   â”‚ scrape_configs    â”‚ Yes         â”‚
â”‚ Alertmanager Auth    â”‚ Sending alerts   â”‚ alerting config   â”‚ Yes         â”‚
â”‚ Remote Write Auth    â”‚ Remote storage   â”‚ remote_write      â”‚ Yes         â”‚
â”‚ Network Security     â”‚ Network level    â”‚ Infrastructure    â”‚ No (external)â”‚
â”‚ Service Mesh         â”‚ mTLS in K8s      â”‚ Infrastructure    â”‚ No (external)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. BEST PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

UI/API Access:
  âœ“ Use reverse proxy for production deployments
  âœ“ Enable TLS for encrypted communication
  âœ“ Use web.config.file for simple setups
  âœ“ Implement strong password policies

Scrape Targets:
  âœ“ Use basic auth or bearer tokens for authenticated targets
  âœ“ Use TLS for encrypted communication
  âœ“ Store credentials securely (secret management)
  âœ“ Use separate credentials per service/job

Alertmanager:
  âœ“ Authenticate connections to Alertmanager
  âœ“ Use bearer tokens for better security
  âœ“ Store tokens securely

Remote Write:
  âœ“ Authenticate all remote write endpoints
  âœ“ Use OAuth2 for cloud services
  âœ“ Rotate credentials regularly

General:
  âœ“ Use network-level security as additional layer
  âœ“ Restrict network access (firewalls, private networks)
  âœ“ Rotate credentials regularly
  âœ“ Use separate credentials per service/job
  âœ“ Store secrets securely (secret management systems)
  âœ“ Monitor authentication failures
  âœ“ Use service accounts in Kubernetes
  âœ“ Implement defense in depth (multiple layers)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. KEY TAKEAWAYS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Prometheus has NO built-in UI/API authentication
âœ“ Authentication must be added via external methods or web.config.file
âœ“ Scrape target authentication IS built-in (basic auth, bearer token, TLS)
âœ“ Alertmanager authentication IS built-in
âœ“ Remote write authentication IS built-in
âœ“ Use reverse proxy for production UI/API access
âœ“ Use web.config.file for simple authentication setups
âœ“ Always use TLS for encrypted communication
âœ“ Store credentials securely
âœ“ Use network-level security as additional layer
âœ“ Rotate credentials regularly
âœ“ Use separate credentials per service/job

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GRAFANA INSTALLATION INSTRUCTIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. INSTALLATION ON macOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Method 1: Using Homebrew (Recommended)
  brew install grafana

  Start Grafana:
  brew services start grafana

  Or run manually:
  grafana-server --config=/usr/local/etc/grafana/grafana.ini

  Default configuration location: /usr/local/etc/grafana/grafana.ini
  Default data directory: /usr/local/var/lib/grafana
  Default log directory: /usr/local/var/log/grafana
  Default port: 3000

  Access Grafana: http://localhost:3000
  Default credentials:
    Username: admin
    Password: admin (you'll be prompted to change on first login)


Method 2: Using Standalone Binary
  # Download Grafana
  wget https://dl.grafana.com/oss/release/grafana-10.x.x.darwin-amd64.tar.gz
  tar -zxvf grafana-10.x.x.darwin-amd64.tar.gz
  cd grafana-10.x.x

  # Run Grafana
  ./bin/grafana-server

  # Or specify config file
  ./bin/grafana-server --config=/path/to/grafana.ini


2. INSTALLATION ON AWS EC2 (Linux/Ubuntu)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Method 1: Using APT Repository (Recommended for Ubuntu/Debian)

  # Update system packages
  sudo apt-get update
  sudo apt-get install -y software-properties-common

  # Add Grafana's official APT repository
  sudo apt-get install -y apt-transport-https
  sudo apt-get install -y software-properties-common wget
  wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -

  # Add Grafana repository
  echo "deb https://packages.grafana.com/oss/deb stable main" | sudo tee -a /etc/apt/sources.list.d/grafana.list

  # Update and install Grafana
  sudo apt-get update
  sudo apt-get install -y grafana

  # Start and enable Grafana service
  sudo systemctl daemon-reload
  sudo systemctl start grafana-server
  sudo systemctl enable grafana-server

  # Check status
  sudo systemctl status grafana-server

  Configuration location: /etc/grafana/grafana.ini
  Data directory: /var/lib/grafana
  Log directory: /var/log/grafana
  Default port: 3000

  Access Grafana: http://<EC2-IP>:3000
  (Make sure security group allows inbound traffic on port 3000)


Method 2: Using Standalone Binary (Linux)

  # Download Grafana
  wget https://dl.grafana.com/oss/release/grafana-10.x.x.linux-amd64.tar.gz
  tar -zxvf grafana-10.x.x.linux-amd64.tar.gz
  cd grafana-10.x.x

  # Create systemd service (optional, for running as service)
  sudo nano /etc/systemd/system/grafana.service

  [Unit]
  Description=Grafana Server
  After=network.target

  [Service]
  Type=simple
  User=grafana
  ExecStart=/path/to/grafana/bin/grafana-server --config=/etc/grafana/grafana.ini
  Restart=on-failure

  [Install]
  WantedBy=multi-user.target

  # Enable and start service
  sudo systemctl daemon-reload
  sudo systemctl enable grafana
  sudo systemctl start grafana


Method 3: Using Docker (Works on any Linux EC2)

  # Pull Grafana image
  docker pull grafana/grafana

  # Run Grafana container
  docker run -d \
    -p 3000:3000 \
    --name=grafana \
    -v grafana-storage:/var/lib/grafana \
    -e "GF_SECURITY_ADMIN_PASSWORD=admin" \
    grafana/grafana

  # Or with custom config file
  docker run -d \
    -p 3000:3000 \
    --name=grafana \
    -v /path/to/grafana.ini:/etc/grafana/grafana.ini \
    -v grafana-storage:/var/lib/grafana \
    grafana/grafana

  # Using docker-compose
  # Create docker-compose.yml:
  version: '3'
  services:
    grafana:
      image: grafana/grafana
      ports:
        - "3000:3000"
      volumes:
        - grafana-storage:/var/lib/grafana
        - ./config/grafana.ini:/etc/grafana/grafana.ini
      environment:
        - GF_SECURITY_ADMIN_PASSWORD=admin

  volumes:
    grafana-storage:


3. INSTALLATION ON AWS (General)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AWS ECS (Elastic Container Service):
  - Use Grafana Docker image in ECS task definition
  - Configure ECS service with appropriate ports and volumes
  - Use ECS service discovery or ALB for access

AWS EKS (Elastic Kubernetes Service):
  - Deploy Grafana using Helm chart:
    helm repo add grafana https://grafana.github.io/helm-charts
    helm repo update
    helm install grafana grafana/grafana

  - Or use Kubernetes manifests:
    kubectl apply -f https://raw.githubusercontent.com/grafana/grafana/main/k8s/grafana-deployment.yaml

AWS Lambda (Grafana Cloud):
  - Use Grafana Cloud for serverless monitoring
  - Not for self-hosted Grafana installation

AWS EC2 with Amazon Linux 2:
  # Install Grafana on Amazon Linux 2
  sudo yum update -y
  sudo yum install -y wget

  # Download and install Grafana RPM
  wget https://dl.grafana.com/oss/release/grafana-10.x.x-1.x86_64.rpm
  sudo yum localinstall -y grafana-10.x.x-1.x86_64.rpm

  # Start Grafana service
  sudo systemctl start grafana-server
  sudo systemctl enable grafana-server


4. POST-INSTALLATION CONFIGURATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Configure Grafana:
  - Edit configuration file (location depends on installation method)
  - Key settings in grafana.ini:
    [server]
    http_port = 3000
    domain = localhost

    [security]
    admin_user = admin
    admin_password = admin

    [database]
    type = sqlite3
    path = grafana.db

    [datasources]
    # Configure Prometheus datasource here or via UI

  - Restart Grafana after configuration changes:
    macOS: brew services restart grafana
    Linux: sudo systemctl restart grafana-server

Add Prometheus as Data Source:
  1. Login to Grafana (http://localhost:3000)
  2. Go to Configuration > Data Sources
  3. Click "Add data source"
  4. Select "Prometheus"
  5. Enter Prometheus URL: http://localhost:9090
  6. Click "Save & Test"

Security Considerations:
  - Change default admin password immediately
  - Configure authentication (LDAP, OAuth, etc.)
  - Use reverse proxy (nginx, Apache) for HTTPS
  - Configure firewall rules
  - Use AWS Security Groups to restrict access
  - Enable SSL/TLS for production


5. VERIFICATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Check if Grafana is running:
  macOS: brew services list | grep grafana
  Linux: sudo systemctl status grafana-server
  Docker: docker ps | grep grafana

Test Grafana API:
  curl http://localhost:3000/api/health

Access Grafana UI:
  http://localhost:3000 (or http://<EC2-IP>:3000 for EC2)

Default login:
  Username: admin
  Password: admin (change on first login)


Docker grafana startup guide: https://github.com/aussiearef/grafana-udemy/tree/main/docker


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GRAFANA DASHBOARD DESIGN PRACTICES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. DASHBOARD LAYOUT & ORGANIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Hierarchy and Structure:
  - Use a top-to-bottom, left-to-right information hierarchy
  - Place most critical metrics at the top
  - Group related metrics together in rows or sections
  - Use row panels to organize and collapse sections
  - Keep dashboard focused on a single purpose (e.g., "Application Overview", "Infrastructure")

Dashboard Sections (Typical Order):
  1. Key Performance Indicators (KPIs) - Top row
  2. System Health - CPU, Memory, Disk, Network
  3. Application Metrics - Request rates, errors, latency
  4. Business Metrics - User activity, transactions
  5. Detailed Breakdowns - Per-service, per-instance views

Panel Sizing:
  - Use consistent panel sizes (e.g., 6x4, 8x4, 12x4)
  - Larger panels for important metrics
  - Smaller panels for supporting context
  - Avoid overcrowding - leave white space

Row Organization:
  - Use row panels to group related metrics
  - Enable row collapse/expand for cleaner view
  - Name rows descriptively (e.g., "CPU Metrics", "Error Rates")


2. PANEL TYPES & VISUALIZATION CHOICES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time Series:
  - Use for metrics that change over time
  - Best for: CPU usage, request rates, error counts, latency
  - Show multiple series with different colors
  - Use stacked area for cumulative metrics
  - Use line graphs for trends

Stat Panels:
  - Use for single-value metrics (KPIs)
  - Best for: Current value, percentage, count
  - Show thresholds with color coding (green/yellow/red)
  - Include sparklines for trend context
  - Use value mappings for meaningful labels

Gauge:
  - Use for metrics with min/max ranges
  - Best for: Percentage utilization, capacity metrics
  - Set appropriate thresholds
  - Use color gradients (green â†’ yellow â†’ red)

Bar Gauge:
  - Use for comparing multiple values
  - Best for: Per-service metrics, top N lists
  - Horizontal for many items, vertical for few

Table:
  - Use for detailed data with multiple dimensions
  - Best for: Top errors, slow queries, per-instance breakdowns
  - Sort by most important column
  - Use color coding for status columns
  - Limit rows (e.g., top 10) for readability

Heatmap:
  - Use for distribution over time
  - Best for: Request latency distribution, error patterns
  - Shows patterns and anomalies clearly

Logs Panel:
  - Use for log exploration
  - Best for: Error logs, access logs, debug information
  - Filter and search capabilities
  - Link to time series panels


3. COLOR SCHEMES & VISUAL DESIGN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Color Consistency:
  - Use consistent colors across panels for same metrics
  - Green = good/healthy, Yellow = warning, Red = critical/error
  - Use colorblind-friendly palettes
  - Avoid too many colors (max 5-7 per panel)

Thresholds:
  - Define clear thresholds for all metrics
  - Use standard ranges (e.g., < 50% = green, 50-80% = yellow, > 80% = red)
  - Make thresholds configurable via variables
  - Document threshold rationale

Visual Clarity:
  - Use clear, descriptive panel titles
  - Include units in panel titles or values
  - Remove unnecessary grid lines and borders
  - Use appropriate font sizes
  - Use legends for multi-series graphs


4. QUERY & DATA BEST PRACTICES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PromQL Queries:
  - Use efficient queries (avoid expensive aggregations)
  - Leverage recording rules for complex calculations
  - Use rate() for counters, increase() for deltas
  - Aggregate at appropriate level (sum, avg, max, min)
  - Use label filters to reduce data volume

Query Performance:
  - Limit time range queries (use $__range variable)
  - Use instant queries for stat panels when possible
  - Avoid queries that scan all metrics
  - Use query inspector to check query performance
  - Cache frequently used queries

Data Transformation:
  - Use transformations to reshape data
  - Calculate derived metrics (e.g., error rate = errors / total)
  - Use reduce transformation for aggregations
  - Use organize fields to rename/hide columns

Aliasing:
  - Use meaningful aliases for series
  - Include instance/service names in aliases
  - Use template variables in aliases
  - Example: {{instance}} - {{job}} instead of default labels


5. DASHBOARD VARIABLES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Variable Types:
  - Query variables: Populate from data source (e.g., list of instances)
  - Custom variables: Manual list of values
  - Text variables: Free-form input
  - Interval variables: Time range options
  - Datasource variables: Switch between data sources

Best Practices:
  - Use variables for filters (instance, job, environment)
  - Provide "All" option for multi-select variables
  - Set sensible defaults
  - Use regex to filter variable options
  - Name variables clearly (e.g., $instance, $environment)

Variable Usage:
  - Use in queries: {instance=~"$instance"}
  - Use in panel titles: "CPU Usage - $instance"
  - Use in repeat panels for multi-instance views
  - Chain variables (dependent variables)


6. TIME RANGES & REFRESH INTERVALS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time Range Selection:
  - Provide common presets (Last 5 minutes, 1 hour, 6 hours, 24 hours, 7 days)
  - Set appropriate default time range
  - Use relative time ranges for consistency
  - Consider data retention when setting ranges

Refresh Intervals:
  - Set refresh based on update frequency
  - High-frequency dashboards: 5s, 10s, 30s
  - Standard dashboards: 1m, 5m
  - Historical/analysis dashboards: Manual refresh
  - Balance between real-time updates and performance


7. ALERTING INTEGRATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Visual Alert Indicators:
  - Show alert status in panels
  - Use color coding to indicate alert state
  - Link panels to alert rules
  - Display alert thresholds on graphs

Alert Annotations:
  - Add alert annotations to time series
  - Show when alerts fired/resolved
  - Include alert message in annotations
  - Use different colors for different alert severities


8. PERFORMANCE OPTIMIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Dashboard Load Time:
  - Limit number of panels (aim for < 20-30 panels)
  - Use query optimization techniques
  - Enable query caching where possible
  - Use data source query limits
  - Consider splitting large dashboards

Panel Refresh:
  - Set appropriate refresh intervals
  - Use "On time range change" for stat panels
  - Avoid unnecessary auto-refresh
  - Use manual refresh for analysis dashboards

Data Source Efficiency:
  - Use appropriate query time ranges
  - Aggregate data at source when possible
  - Use recording rules for expensive queries
  - Limit label cardinality


9. DOCUMENTATION & METADATA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Panel Descriptions:
  - Add descriptions to explain what metrics show
  - Include calculation formulas if applicable
  - Document data source and collection method
  - Explain thresholds and alert conditions

Dashboard Annotations:
  - Use text panels for dashboard notes
  - Document dashboard purpose and scope
  - Include links to related dashboards
  - Add changelog or version info

Tags and Organization:
  - Use descriptive dashboard names
  - Add tags for easy discovery (e.g., "production", "infrastructure", "application")
  - Organize in folders by team/service
  - Add dashboard descriptions


10. ACCESSIBILITY & USABILITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Accessibility:
  - Use high contrast colors
  - Ensure text is readable
  - Use descriptive panel titles
  - Provide alternative text for visualizations
  - Test with screen readers

Mobile Responsiveness:
  - Test dashboard on mobile devices
  - Use appropriate panel sizes for mobile
  - Consider creating mobile-specific dashboards
  - Use stat panels for mobile (easier to read)

User Experience:
  - Make dashboards self-explanatory
  - Use consistent naming conventions
  - Provide tooltips and help text
  - Include links to related resources
  - Make dashboards shareable and exportable


11. DASHBOARD TEMPLATES & REUSABILITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Template Dashboards:
  - Create reusable dashboard templates
  - Use variables for customization
  - Export/import dashboards as JSON
  - Version control dashboard JSON files
  - Share templates across teams

Panel Library:
  - Save commonly used panels as library panels
  - Reuse panels across dashboards
  - Update library panels to propagate changes
  - Organize library panels by category


12. SECURITY & PERMISSIONS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Access Control:
  - Set appropriate dashboard permissions
  - Use folder-level permissions
  - Restrict edit access to authorized users
  - Use view-only permissions for most users

Data Security:
  - Don't expose sensitive data in dashboards
  - Use data source permissions
  - Sanitize labels and metric names
  - Consider data retention policies


13. COMMON PATTERNS & ANTI-PATTERNS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Good Patterns:
  âœ“ Single-purpose dashboards
  âœ“ Consistent panel sizing
  âœ“ Clear visual hierarchy
  âœ“ Meaningful variable names
  âœ“ Documented thresholds
  âœ“ Efficient queries
  âœ“ Appropriate visualization types
  âœ“ Mobile-friendly layouts

Anti-Patterns to Avoid:
  âœ— Too many panels (> 30)
  âœ— Inconsistent color schemes
  âœ— Unclear panel titles
  âœ— Expensive queries without optimization
  âœ— No documentation
  âœ— Mixing unrelated metrics
  âœ— Overly complex visualizations
  âœ— Hard-coded values instead of variables
  âœ— No alert integration
  âœ— Poor mobile experience


14. DASHBOARD TESTING & VALIDATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Testing Checklist:
  - Verify all queries return data
  - Check panel calculations are correct
  - Validate thresholds are appropriate
  - Test variable filters work correctly
  - Ensure dashboard loads in reasonable time
  - Test on different screen sizes
  - Verify alert links work
  - Check time range presets
  - Validate refresh intervals
  - Test with different user permissions


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
WHAT CAN BE MONITORED IN GRAFANA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Grafana can monitor virtually anything that exposes metrics, logs, or traces. It supports
100+ data sources and can visualize data from various systems, applications, and services.


1. INFRASTRUCTURE MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Server/System Metrics:
  - CPU Usage: Per core, overall utilization, load average
  - Memory: Used, free, cached, swap usage
  - Disk I/O: Read/write rates, IOPS, disk space, disk latency
  - Network: Bandwidth, packets, errors, connections, network latency
  - System Load: Load average, uptime, boot time
  - Process Metrics: Running processes, process counts, resource usage per process

Virtualization & Containers:
  - Docker: Container metrics, resource usage, container health
  - Kubernetes: Pod metrics, node metrics, cluster health, resource quotas
  - VM Metrics: Virtual machine CPU, memory, disk, network
  - Hypervisor Metrics: Host-level virtualization metrics

Cloud Infrastructure:
  - AWS: EC2, ECS, EKS, RDS, S3, CloudWatch metrics
  - Azure: Virtual Machines, AKS, SQL Database, Blob Storage
  - GCP: Compute Engine, GKE, Cloud SQL, Cloud Storage
  - CloudWatch, Azure Monitor, Google Cloud Monitoring integration


2. APPLICATION MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Application Performance:
  - Request Rates: HTTP requests per second, API calls
  - Response Times: P50, P95, P99 latency percentiles
  - Error Rates: HTTP error codes (4xx, 5xx), application errors
  - Throughput: Transactions per second, operations per second
  - Active Users: Concurrent users, session counts

Application Metrics:
  - Business Logic Metrics: Custom application counters, gauges
  - Database Queries: Query performance, slow queries, connection pools
  - Cache Metrics: Hit/miss rates, cache size, eviction rates
  - Queue Metrics: Queue depth, processing rates, wait times
  - Job Processing: Job completion rates, job duration, failures

Programming Language Specific:
  - Java: JVM metrics (heap, GC, threads), JMX metrics
  - Python: Application metrics, Django/Flask specific metrics
  - Node.js: Event loop lag, memory usage, request metrics
  - Go: Goroutines, GC pauses, memory allocations
  - .NET: CLR metrics, garbage collection, thread pool


3. DATABASE MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SQL Databases:
  - PostgreSQL: Connections, queries, replication lag, table sizes
  - MySQL/MariaDB: Query performance, slow queries, connections
  - SQL Server: Database size, query execution times, locks
  - Oracle: Tablespace usage, session counts, wait events

NoSQL Databases:
  - MongoDB: Operations, connections, replication lag, sharding metrics
  - Cassandra: Read/write latencies, compaction, node health
  - Redis: Memory usage, hit/miss rates, commands per second
  - Elasticsearch: Index size, search latency, cluster health
  - InfluxDB: Series count, write throughput, query performance

Database Performance:
  - Query Performance: Slow queries, query execution time
  - Connection Pools: Active connections, connection wait times
  - Replication: Replication lag, replication status
  - Backup Status: Backup completion, backup duration
  - Database Size: Table sizes, index sizes, growth rates


4. NETWORK MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Network Devices:
  - Routers: Interface utilization, packet drops, errors
  - Switches: Port statistics, bandwidth, errors
  - Firewalls: Connection counts, blocked traffic, rule hits
  - Load Balancers: Request distribution, health checks, SSL metrics

Network Metrics:
  - Bandwidth: Inbound/outbound traffic, utilization
  - Latency: Round-trip time, network delays
  - Packet Loss: Dropped packets, error rates
  - Connections: Active connections, connection rates
  - DNS: Query latency, DNS resolution times


5. WEB SERVER MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Web Servers:
  - Nginx: Request rates, active connections, upstream response times
  - Apache: Requests per second, worker threads, CPU usage
  - IIS: Request counts, response times, application pool metrics

Web Application Metrics:
  - HTTP Metrics: Status codes, request methods, response sizes
  - SSL/TLS: Certificate expiration, SSL handshake times
  - CDN Metrics: Cache hit rates, origin requests, bandwidth
  - API Gateway: API calls, throttling, authentication failures


6. MESSAGE QUEUE & STREAMING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Message Queues:
  - RabbitMQ: Queue depth, message rates, consumer lag
  - Apache Kafka: Topic offsets, consumer lag, throughput
  - Amazon SQS: Queue depth, message age, dead letter queues
  - ActiveMQ: Queue sizes, message rates, broker health

Streaming Platforms:
  - Kafka Streams: Processing rates, lag, error rates
  - Apache Flink: Checkpoint duration, throughput, backpressure
  - Apache Spark: Job duration, task completion, resource usage


7. LOG MONITORING (with Loki)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Log Aggregation:
  - Application Logs: Error logs, access logs, debug logs
  - System Logs: syslog, systemd logs, kernel logs
  - Security Logs: Authentication logs, audit logs, intrusion attempts
  - Container Logs: Docker logs, Kubernetes pod logs

Log Analysis:
  - Error Patterns: Error frequency, error types, error trends
  - Log Volume: Log rate, log size, retention metrics
  - Search & Filter: Log queries, log exploration
  - Correlation: Correlate logs with metrics and traces


8. DISTRIBUTED TRACING (with Tempo/Jaeger)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Trace Metrics:
  - Request Traces: End-to-end request tracing
  - Service Dependencies: Service map, dependency graph
  - Span Metrics: Span duration, span counts, error spans
  - Trace Sampling: Sampling rates, trace coverage

Performance Analysis:
  - Service Latency: Service-level latency breakdowns
  - Critical Path: Identify bottlenecks in request flow
  - Error Tracing: Trace errors through distributed systems
  - Database Query Tracing: Database query performance in traces


9. BUSINESS METRICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Business KPIs:
  - Revenue Metrics: Sales, revenue, transaction volume
  - User Metrics: Active users, new users, user retention
  - Conversion Metrics: Conversion rates, funnel analysis
  - Engagement Metrics: Page views, session duration, clicks

E-commerce:
  - Order Metrics: Orders per hour, order value, cart abandonment
  - Product Metrics: Product views, add-to-cart rates, sales by product
  - Payment Metrics: Payment success rates, payment processing time

SaaS Metrics:
  - Subscription Metrics: MRR, churn rate, customer lifetime value
  - Feature Usage: Feature adoption, feature usage rates
  - Support Metrics: Ticket volume, resolution time, customer satisfaction


10. SECURITY MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Security Events:
  - Authentication: Login attempts, failed logins, account lockouts
  - Authorization: Permission denials, access violations
  - Intrusion Detection: Suspicious activities, attack patterns
  - Firewall Events: Blocked connections, security rule hits

Security Metrics:
  - Threat Detection: Malware detection, vulnerability scans
  - Compliance: Compliance status, policy violations
  - Access Patterns: Unusual access patterns, privilege escalations
  - Security Incidents: Incident counts, incident response time


11. CI/CD PIPELINE MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Build & Deployment:
  - Build Metrics: Build duration, build success rates, build frequency
  - Deployment Metrics: Deployment frequency, deployment duration
  - Test Metrics: Test execution time, test pass/fail rates, coverage
  - Pipeline Health: Pipeline success rates, stage durations

Version Control:
  - Git Metrics: Commit frequency, code review time, merge rates
  - Repository Metrics: Repository size, branch counts, pull request metrics


12. STORAGE MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Storage Systems:
  - File Systems: Disk usage, I/O rates, file counts
  - Object Storage: S3 buckets, blob storage, object counts
  - Block Storage: Volume usage, IOPS, throughput
  - Network Storage: NFS, CIFS performance metrics

Backup & Recovery:
  - Backup Status: Backup completion, backup duration, backup size
  - Recovery Metrics: Recovery time, recovery point objectives
  - Storage Growth: Storage growth rates, capacity planning


13. CLOUD SERVICES MONITORING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AWS Services:
  - EC2: Instance metrics, auto-scaling, spot instances
  - RDS: Database performance, replication, backups
  - S3: Bucket metrics, request rates, storage classes
  - Lambda: Invocations, duration, errors, throttles
  - CloudFront: Cache hit rates, origin requests, bandwidth
  - API Gateway: API calls, latency, 4xx/5xx errors

Azure Services:
  - Virtual Machines: CPU, memory, disk, network
  - Azure SQL: DTU usage, query performance, connections
  - Blob Storage: Storage usage, access patterns
  - Functions: Execution count, duration, errors

GCP Services:
  - Compute Engine: Instance metrics, autoscaling
  - Cloud SQL: Database performance, connections
  - Cloud Storage: Storage usage, operations
  - Cloud Functions: Invocations, execution time


14. CUSTOM METRICS & BUSINESS LOGIC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Custom Application Metrics:
  - Business Events: User actions, feature usage, conversions
  - Workflow Metrics: Process completion, step durations
  - Inventory Metrics: Stock levels, inventory turnover
  - Financial Metrics: Revenue, costs, profit margins

IoT & Edge Devices:
  - Sensor Data: Temperature, humidity, pressure, motion
  - Device Health: Battery levels, connectivity, device status
  - Telemetry: GPS coordinates, device location, movement
  - Industrial IoT: Machine metrics, production rates, quality metrics


15. SUPPORTED DATA SOURCES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Time Series Databases:
  - Prometheus (most common)
  - InfluxDB
  - TimescaleDB
  - Graphite
  - OpenTSDB
  - Azure Data Explorer

Log Aggregation:
  - Loki (Grafana's log aggregation system)
  - Elasticsearch
  - Splunk
  - CloudWatch Logs

Tracing:
  - Tempo (Grafana's tracing backend)
  - Jaeger
  - Zipkin
  - OpenTelemetry

Cloud Providers:
  - AWS CloudWatch
  - Azure Monitor
  - Google Cloud Monitoring
  - Alibaba Cloud

Databases:
  - PostgreSQL
  - MySQL
  - SQL Server
  - MongoDB
  - Redis
  - Oracle

Monitoring Tools:
  - Datadog
  - New Relic
  - Dynatrace
  - AppDynamics
  - Zabbix
  - Nagios

Other:
  - JSON API
  - CSV files
  - TestData DB (for testing)
  - And 100+ more data sources via plugins


16. COMMON MONITORING USE CASES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Infrastructure Dashboards:
  - Server health overview
  - Kubernetes cluster monitoring
  - Cloud resource utilization
  - Network performance

Application Dashboards:
  - Application performance (APM)
  - API performance monitoring
  - Microservices health
  - Service-level objectives (SLOs)

Business Dashboards:
  - Revenue and sales metrics
  - User engagement metrics
  - Business KPI tracking
  - Executive dashboards

Operational Dashboards:
  - On-call dashboards
  - Incident response dashboards
  - Capacity planning
  - Cost optimization

Security Dashboards:
  - Security event monitoring
  - Threat detection
  - Compliance monitoring
  - Access audit logs


=============================================================================

Loki - Log Aggregation System
==============================

Overview:
---------
Loki is a horizontally-scalable, highly-available log aggregation system inspired by Prometheus.
It's designed to be cost-effective and operationally simple, using labels to index logs rather
than full-text indexing like traditional log aggregation systems.

Key Characteristics:
- Label-based indexing (similar to Prometheus metrics)
- Stores compressed, unstructured logs
- Integrates seamlessly with Grafana
- Uses object storage (S3, GCS, Azure Blob, etc.) for persistence
- Lower cost and simpler operations compared to full-text search systems

Use Cases:
----------
1. Centralized Logging:
   - Aggregate logs from multiple services/applications
   - Unified log querying across distributed systems
   - Container and Kubernetes log collection

2. Troubleshooting & Debugging:
   - Investigate application errors and exceptions
   - Trace request flows across microservices
   - Debug performance issues and bottlenecks

3. Security & Compliance:
   - Security event log analysis
   - Audit trail monitoring
   - Compliance log retention
   - Anomaly detection in log patterns

4. Application Monitoring:
   - Application health monitoring via logs
   - Error rate tracking
   - User activity tracking
   - Business event logging

5. Integration with Observability Stack:
   - Correlate logs with metrics (Prometheus) and traces (Tempo)
   - Unified observability in Grafana
   - Full-stack troubleshooting workflows

Best Practices:
---------------
1. Label Strategy:
   - Use high-cardinality labels sparingly (job, instance, level)
   - Avoid using unique values (user IDs, request IDs) as labels
   - Keep label sets small and consistent
   - Use LogQL for filtering instead of labels

2. Log Retention:
   - Set appropriate retention policies based on compliance needs
   - Use object storage lifecycle policies for cost optimization
   - Archive old logs to cheaper storage tiers

3. Performance:
   - Batch log writes to reduce overhead
   - Use Promtail (Loki's agent) for efficient log shipping
   - Configure appropriate chunk sizes and retention
   - Monitor Loki's own metrics for performance

4. Query Optimization:
   - Use LogQL filters early in queries to reduce data scanned
   - Leverage log range queries for time-series analysis
   - Use regex patterns efficiently
   - Cache frequently used queries

5. Architecture:
   - Deploy Loki in microservices mode for scalability
   - Use separate components: distributor, ingester, querier, compactor
   - Implement proper authentication and authorization
   - Use multi-tenancy for isolation between teams/apps

6. Integration:
   - Use Promtail for Kubernetes log collection
   - Configure proper service discovery
   - Set up log relabeling for consistent labels
   - Integrate with Grafana for visualization

7. Security:
   - Enable authentication (basic auth, OAuth, etc.)
   - Use TLS for all communications
   - Implement RBAC for multi-tenant environments
   - Encrypt sensitive log data at rest

8. Cost Management:
   - Compress logs before storage
   - Use object storage with lifecycle policies
   - Set appropriate retention periods
   - Monitor storage usage and optimize chunk sizes

==============================================================================

Grafana Alloy - Observability Agent
====================================

Overview:
---------
Grafana Alloy is a vendor-neutral, open-source observability agent that collects and forwards
telemetry data (metrics, logs, traces) to various backends. It's the successor to Grafana Agent
and is built on the OpenTelemetry Collector framework. Alloy uses a declarative configuration
language called River to define data collection and routing pipelines.

Key Characteristics:
- Vendor-neutral: Works with Prometheus, Loki, Tempo, Mimir, and other backends
- Component-based architecture: Modular components for different data sources
- River configuration language: Declarative, type-safe configuration
- Built on OpenTelemetry: Leverages OTel Collector framework
- Single binary: Easy to deploy and manage
- Supports multiple protocols: Prometheus, OpenTelemetry, Loki, etc.

Use Cases:
----------
1. Metrics Collection:
   - Scrape Prometheus metrics from applications
   - Forward metrics to Prometheus, Mimir, or other backends
   - Service discovery for dynamic targets
   - Metrics relabeling and transformation

2. Log Collection:
   - Collect logs from files, syslog, journald, etc.
   - Forward logs to Loki or other log aggregation systems
   - Log parsing and transformation
   - Multi-line log handling

3. Distributed Tracing:
   - Collect traces from applications (OTLP, Jaeger, Zipkin)
   - Forward traces to Tempo or other trace backends
   - Trace sampling and filtering
   - Service map generation

4. Multi-Backend Routing:
   - Route same data to multiple backends simultaneously
   - Different retention policies per backend
   - Backup and redundancy strategies
   - Multi-cloud observability

5. Edge/Remote Monitoring:
   - Deploy at edge locations
   - Collect data from remote sites
   - Forward to central observability stack
   - Bandwidth optimization

6. Kubernetes Integration:
   - DaemonSet for node-level collection
   - Sidecar pattern for pod-level collection
   - Service discovery for pods and services
   - Automatic label injection

Alloy vs Loki - Comparison:
----------------------------
These are COMPLEMENTARY tools, not competitors:

Alloy (Agent/Collector):
- Purpose: Data collection and forwarding agent
- Role: Runs on hosts/containers to collect telemetry
- Function: Scrapes, collects, transforms, and routes data
- Output: Sends data to backends (Loki, Prometheus, Tempo, etc.)
- Deployment: Deployed alongside applications or as DaemonSet
- Configuration: River language for data pipelines

Loki (Storage/Query Engine):
- Purpose: Log aggregation and storage system
- Role: Backend storage and query engine for logs
- Function: Stores logs, indexes by labels, provides LogQL queries
- Input: Receives logs from agents like Alloy/Promtail
- Deployment: Centralized service in observability stack
- Configuration: YAML for storage and query configuration

Relationship:
- Alloy collects logs â†’ forwards to Loki â†’ Loki stores and indexes â†’ Grafana queries Loki
- Alloy can replace Promtail as the log shipper to Loki
- Alloy provides more flexibility than Promtail (can route to multiple backends)
- Alloy handles metrics and traces too, while Promtail is log-only

When to Use:
- Use Alloy: When you need unified agent for metrics, logs, and traces
- Use Promtail: When you only need log shipping to Loki (simpler, lighter)
- Use Loki: Always needed as the log storage backend (works with both Alloy and Promtail)

Best Practices:
---------------
1. Configuration Management:
   - Use River configuration files (not YAML like Grafana Agent)
   - Version control all Alloy configurations
   - Use environment variables for sensitive data
   - Modularize configurations for reusability

2. Component Selection:
   - Use appropriate components for data sources (prometheus.scrape, loki.source.file, etc.)
   - Leverage built-in components before writing custom ones
   - Understand component lifecycle and error handling
   - Use component health checks

3. Resource Management:
   - Set appropriate scrape intervals to balance freshness vs load
   - Configure timeouts for all components
   - Limit cardinality at collection time (not just at storage)
   - Use sampling for high-volume traces

4. Labeling Strategy:
   - Add consistent labels at collection time
   - Use relabeling to standardize labels across sources
   - Avoid high-cardinality labels (same as Loki best practices)
   - Leverage Kubernetes service discovery for automatic labels

5. Error Handling:
   - Configure retry policies for failed forwards
   - Set up dead letter queues for undeliverable data
   - Monitor Alloy's own metrics for health
   - Log Alloy errors to a separate location

6. Performance:
   - Batch writes to backends (configure batch sizes)
   - Use compression for network efficiency
   - Deploy Alloy close to data sources (reduce network hops)
   - Scale horizontally for high-volume environments

7. Security:
   - Use TLS for all backend connections
   - Authenticate with backends (API keys, mTLS, etc.)
   - Encrypt sensitive data in transit
   - Use RBAC for configuration access
   - Rotate credentials regularly

8. Multi-Backend Strategy:
   - Route critical data to multiple backends for redundancy
   - Use different retention policies per backend
   - Route to staging/test environments for validation
   - Implement data filtering per backend

9. Kubernetes Deployment:
   - Use DaemonSet for node-level collection
   - Use sidecar pattern for application-specific collection
   - Leverage ConfigMaps/Secrets for configuration
   - Use service accounts with appropriate RBAC
   - Monitor resource limits (CPU/memory)

10. Monitoring Alloy Itself:
    - Expose Alloy's internal metrics (Prometheus format)
    - Monitor component health and errors
    - Track forward success/failure rates
    - Alert on configuration reload failures
    - Monitor resource usage (CPU, memory, network)

11. Migration from Grafana Agent:
    - Alloy uses River config (not YAML)
    - Convert existing Agent configs to River
    - Test configurations in staging first
    - Gradual migration approach recommended
    - Alloy is backward compatible with Agent's functionality

12. Integration Patterns:
    - Use Alloy with Loki for logs
    - Use Alloy with Prometheus/Mimir for metrics
    - Use Alloy with Tempo for traces
    - Combine all three for full observability stack
    - Use Grafana to visualize data from all backends

================================================================================

STATIC LABELS IN PROMTAIL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What are Static Labels?
- Static labels are key-value pairs that are added to ALL log entries from a 
  particular scrape configuration
- They are defined in the `labels` section of a `static_configs` entry
- These labels are applied at scrape time and remain constant for all logs 
  collected by that configuration
- They help identify the source, environment, or type of logs in Loki

Configuration Structure:
  scrape_configs:
    - job_name: "system"
      static_configs:
        - targets:
            - localhost
          labels:
            job: varlogs          # Static label
            environment: prod     # Static label
            host: server-01       # Static label
            __path__: /var/log/*log  # Special label for file path

Key Points:
1. Static labels are applied to every log line from that scrape config
2. They appear in Loki queries and can be used for filtering/logql queries
3. Labels starting with `__` are internal/system labels (like `__path__`)
4. Static labels are merged with any labels added by relabeling rules
5. Labels are indexed in Loki, so use them for common filtering needs

Common Static Labels:
- job: Identifies the job/scrape config (e.g., "varlogs", "app-logs")
- environment: Environment name (e.g., "prod", "staging", "dev")
- host: Hostname or server identifier
- service: Service or application name
- team: Team responsible for the logs
- region: Geographic or logical region

Example Configuration:
  scrape_configs:
    - job_name: "application-logs"
      static_configs:
        - targets:
            - localhost
          labels:
            job: myapp
            environment: production
            service: api-server
            team: backend
            region: us-east-1
            __path__: /var/log/app/*.log
    
    - job_name: "system-logs"
      static_configs:
        - targets:
            - localhost
          labels:
            job: system
            environment: production
            __path__: /var/log/syslog

Using Static Labels in LogQL:
- Query logs by static label: {job="myapp"}
- Combine with other labels: {job="myapp", environment="production"}
- Filter by multiple static labels: {service="api-server", team="backend"}

Best Practices:
1. Use meaningful label names that describe the log source
2. Keep label cardinality low (avoid high-cardinality values like timestamps)
3. Use consistent label names across different scrape configs
4. Prefer static labels for values that don't change per log line
5. Use relabeling for dynamic labels that need to be extracted from log content
6. Avoid using too many static labels (Loki indexes all labels)

Static Labels vs Dynamic Labels:
- Static Labels: Defined in config, same for all logs in scrape config
- Dynamic Labels: Extracted from log content using relabeling or pipeline stages
- Static labels are more efficient for filtering in Loki queries

Special Labels:
- __path__: Required for file-based scraping, specifies file path pattern
- __hostname__: Can be used to add hostname automatically
- __job__: Automatically set to job_name if not explicitly defined
============================================================================

DYNAMIC LABELS USING PIPELINE_STAGES IN PROMTAIL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What are Pipeline Stages?
- Pipeline stages are processing steps that transform log entries before sending 
  them to Loki
- They allow extraction of dynamic labels from log content (not just static values)
- Each stage processes the log entry in sequence, modifying it as it passes through
- Labels extracted via pipeline stages are added to the log entry and indexed in Loki

Key Concept:
- Static labels: Same for all logs in a scrape config (defined in static_configs)
- Dynamic labels: Extracted from log content per log line (using pipeline_stages)
- Dynamic labels allow filtering by values that vary between log entries

Pipeline Stages Overview:
1. docker: Parse Docker log format
2. cri: Parse CRI (Container Runtime Interface) log format
3. json: Parse JSON log entries
4. regex: Extract values using regular expressions
5. timestamp: Parse and set timestamp from log content
6. labels: Add labels from extracted values
7. output: Modify the log line output
8. match: Conditionally apply stages based on filters
9. template: Transform log entries using Go templates
10. multiline: Combine multiple lines into single log entry
11. drop: Drop log entries based on conditions
12. limit: Rate limit log entries

Common Pipeline Stages for Dynamic Labels:

1. JSON PARSING STAGE:
   Extracts fields from JSON log entries and converts them to labels
   
   Example:
     pipeline_stages:
       - json:
           expressions:
             level: level          # Extract 'level' field as 'level' label
             user_id: user.id      # Extract nested 'user.id' as 'user_id' label
             status_code: status   # Extract 'status' field as 'status_code' label
       - labels:
           level:                  # Convert extracted value to label
           user_id:                # Convert extracted value to label
           status_code:            # Convert extracted value to label
   
   Input log: {"level":"error","user":{"id":"12345"},"status":500}
   Result: Labels added: level="error", user_id="12345", status_code="500"

2. REGEX EXTRACTION STAGE:
   Uses regular expressions to extract values from log lines
   
   Example:
     pipeline_stages:
       - regex:
           expression: '.*level=(?P<level>\w+).*user_id=(?P<user_id>\d+).*'
       - labels:
           level:
           user_id:
   
   Input log: "ERROR: level=error user_id=12345 request failed"
   Result: Labels added: level="error", user_id="12345"
   
   Named capture groups become extracted values that can be converted to labels

3. DOCKER LOG PARSING:
   Parses Docker log format (includes timestamp and log level)
   
   Example:
     pipeline_stages:
       - docker: {}
       - labels:
           level:                  # Extract log level from Docker format

4. CRI LOG PARSING:
   Parses CRI (Container Runtime Interface) log format
   
   Example:
     pipeline_stages:
       - cri: {}
       - labels:
           level:

5. TEMPLATE STAGE:
   Uses Go templates to transform log entries and extract values
   
   Example:
     pipeline_stages:
       - template:
           source: extracted
           template: '{{ .level }}-{{ .service }}'
           output: log_level_service
       - labels:
           log_level_service:

6. MATCH STAGE (Conditional Processing):
   Applies stages only when conditions are met
   
   Example:
     pipeline_stages:
       - match:
           selector: '{job="api"}'
           stages:
             - json:
                 expressions:
                   endpoint: path
             - labels:
                 endpoint:
       - match:
           selector: '{job="system"}'
           stages:
             - regex:
                 expression: 'ERROR: (?P<error_type>\w+)'
             - labels:
                 error_type:

Complete Example Configuration:

  scrape_configs:
    - job_name: "application-logs"
      static_configs:
        - targets:
            - localhost
          labels:
            job: myapp
            environment: production
            __path__: /var/log/app/*.log
      pipeline_stages:
        # Parse JSON logs
        - json:
            expressions:
              level: level
              service: service
              trace_id: trace_id
              user_id: user.id
              status_code: status
              method: method
              path: path
        
        # Convert extracted JSON fields to labels
        - labels:
            level:                    # Log level (info, error, warn, debug)
            service:                  # Service name
            trace_id:                 # Distributed tracing ID
            status_code:              # HTTP status code
        
        # Only add user_id label if it exists (avoid high cardinality)
        - match:
            selector: '{level="error"}'
            stages:
              - labels:
                  user_id:            # Only label errors with user_id
        
        # Parse timestamp from log if present
        - timestamp:
            source: timestamp
            format: RFC3339
        
        # Drop debug logs in production
        - match:
            selector: '{level="debug"}'
            action: drop

Alternative: Regex for Non-JSON Logs:

  pipeline_stages:
    # Extract from structured text logs
    - regex:
        expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>\w+)\] (?P<message>.*)$'
    - timestamp:
        source: timestamp
        format: '2006-01-02 15:04:05'
    - labels:
        level:

Input: "2024-01-15 10:30:45 [ERROR] Database connection failed"
Result: Labels: level="ERROR", timestamp parsed correctly

Best Practices for Dynamic Labels:

1. Keep Label Cardinality Low:
   - Avoid extracting high-cardinality values (user IDs, request IDs) as labels
   - Use them in log content instead, query with LogQL filters when needed
   - Example: Don't label every unique user_id, only label on errors

2. Use Match Stages for Conditional Labeling:
   - Only extract labels when needed (e.g., error logs)
   - Reduces label cardinality and improves performance

3. Extract Common Filter Values:
   - Good labels: level, service, environment, region
   - Bad labels: timestamp, request_id, session_id (too many unique values)

4. Combine Static and Dynamic Labels:
   - Use static labels for constant values (environment, team)
   - Use dynamic labels for values that vary per log (level, status_code)

5. Validate Extracted Values:
   - Use regex validation or drop invalid entries
   - Prevents malformed labels in Loki

6. Performance Considerations:
   - Each pipeline stage adds processing overhead
   - Order stages efficiently (parse first, then extract)
   - Use match stages to skip unnecessary processing

Example: High-Cardinality Anti-Pattern:

  # BAD: Labels every unique request_id
  pipeline_stages:
    - json:
        expressions:
          request_id: request_id
    - labels:
        request_id:              # Creates thousands of label combinations!

  # GOOD: Only label errors, keep request_id in log content
  pipeline_stages:
    - json:
        expressions:
          request_id: request_id
          level: level
    - match:
        selector: '{level="error"}'
        stages:
          - labels:
              level:             # Only label level, query request_id in LogQL

Querying Dynamic Labels in LogQL:
- Query by dynamic label: {level="error"}
- Combine static and dynamic: {job="myapp", level="error"}
- Filter by multiple dynamic labels: {level="error", status_code="500"}
- Use in aggregations: sum(count_over_time({level="error"}[5m]))

Pipeline Stage Order Matters:
1. Parse format first (json, regex, docker, cri)
2. Extract values (expressions in json/regex)
3. Convert to labels (labels stage)
4. Transform/modify (template, output)
5. Filter/drop (match, drop)

Common Patterns:

Pattern 1: JSON Application Logs
  pipeline_stages:
    - json:
        expressions:
          level: level
          service: service
    - labels:
        level:
        service:

Pattern 2: Structured Text Logs
  pipeline_stages:
    - regex:
        expression: '\[(?P<level>\w+)\].*service=(?P<service>\w+)'
    - labels:
        level:
        service:

Pattern 3: Conditional Error Labeling
  pipeline_stages:
    - json:
        expressions:
          level: level
          error_type: error.type
    - labels:
        level:
    - match:
        selector: '{level="error"}'
        stages:
          - labels:
              error_type:

Pattern 4: Multi-line Logs
  pipeline_stages:
    - multiline:
        firstline: '^\d{4}-\d{2}-\d{2}'
        max_wait_time: 3s
    - regex:
        expression: '^(?P<timestamp>\d{4}-\d{2}-\d{2}).*\[(?P<level>\w+)\]'
    - timestamp:
        source: timestamp
        format: '2006-01-02'
    - labels:
        level:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OPENTELEMETRY (OTEL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is OpenTelemetry?
- OpenTelemetry is an open-source observability framework that provides a 
  unified set of APIs, SDKs, and tools for collecting telemetry data
- It standardizes how applications generate and export metrics, logs, and traces
- Vendor-neutral: Works with any observability backend (Prometheus, Grafana, 
  Datadog, New Relic, etc.)
- Part of Cloud Native Computing Foundation (CNCF)
- Replaces vendor-specific instrumentation with a single, standardized approach

Core Components:
1. APIs: Language-specific APIs for instrumentation
2. SDKs: Language-specific implementations of the APIs
3. Instrumentation Libraries: Auto-instrumentation for common frameworks
4. Collector: Receives, processes, and exports telemetry data
5. Exporters: Send data to various backends (OTLP, Prometheus, Jaeger, etc.)

Three Pillars of Observability:
1. Metrics: Numerical measurements over time (CPU, memory, request rate)
2. Logs: Event records with timestamps and contextual information
3. Traces: Request flows through distributed systems

Use Cases:
1. Distributed Tracing:
   - Track requests across microservices
   - Identify bottlenecks and latency issues
   - Understand service dependencies
   - Debug performance problems in complex architectures

2. Application Metrics:
   - Business metrics (orders, revenue, user actions)
   - Application performance metrics (response times, error rates)
   - Infrastructure metrics (CPU, memory, network)
   - Custom application-specific metrics

3. Log Correlation:
   - Correlate logs with traces using trace IDs
   - Add context to logs (user ID, request ID, trace ID)
   - Unified logging across services

4. Service Mesh Observability:
   - Instrument service mesh (Istio, Linkerd) traffic
   - Monitor inter-service communication
   - Track service-to-service latency

5. Cloud-Native Applications:
   - Kubernetes-native observability
   - Container and pod-level metrics
   - Multi-cloud and hybrid deployments

6. Full-Stack Observability:
   - Frontend (browser) to backend (server) tracing
   - Database query tracing
   - Third-party API call tracking

Installation Instructions:

1. OpenTelemetry Collector:
   # Using Docker
   docker run -p 4317:4317 -p 4318:4318 \
     -v $(pwd)/otel-collector-config.yaml:/etc/otelcol/config.yaml \
     otel/opentelemetry-collector:latest

   # Using Homebrew (macOS)
   brew install opentelemetry-collector

   # Download binary
   wget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.91.0/otelcol_0.91.0_darwin_amd64.tar.gz
   tar -xzf otelcol_0.91.0_darwin_amd64.tar.gz
   ./otelcol --config=config.yaml

2. Language-Specific SDKs:

   Python:
     pip install opentelemetry-api
     pip install opentelemetry-sdk
     pip install opentelemetry-instrumentation
     pip install opentelemetry-exporter-otlp

   Node.js/JavaScript:
     npm install @opentelemetry/api
     npm install @opentelemetry/sdk-node
     npm install @opentelemetry/instrumentation

   Java:
     <!-- Maven -->
     <dependency>
       <groupId>io.opentelemetry</groupId>
       <artifactId>opentelemetry-api</artifactId>
       <version>1.32.0</version>
     </dependency>

   Go:
     go get go.opentelemetry.io/otel
     go get go.opentelemetry.io/otel/sdk
     go get go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc

   .NET/C#:
     dotnet add package OpenTelemetry
     dotnet add package OpenTelemetry.Exporter.Console
     dotnet add package OpenTelemetry.Exporter.OTLP

3. Auto-Instrumentation:
   # Python auto-instrumentation
   pip install opentelemetry-instrumentation
   opentelemetry-instrument --traces_exporter otlp \
     --metrics_exporter otlp \
     python app.py

   # Java auto-instrumentation
   java -javaagent:opentelemetry-javaagent.jar \
     -Dotel.service.name=my-service \
     -Dotel.traces.exporter=otlp \
     -jar app.jar

OPENTELEMETRY COLLECTOR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is the OpenTelemetry Collector?
- A vendor-neutral agent that receives, processes, and exports telemetry data
- Acts as a proxy between applications and observability backends
- Handles data transformation, filtering, batching, and routing
- Can be deployed as a sidecar, daemonset, or standalone service

Collector Architecture:
  Application â†’ OTLP â†’ Collector â†’ [Processors] â†’ Exporters â†’ Backend
                    (Receivers)    (Processors)   (Exporters)

Components:
1. Receivers: Receive telemetry data from various sources
2. Processors: Transform, filter, and batch data
3. Exporters: Send data to various backends
4. Extensions: Provide additional functionality (health checks, pprof)

Collector Types:
1. Core Collector (otelcol):
   - Minimal set of components
   - Lightweight, production-ready
   - Recommended for most use cases

2. Contrib Collector (otelcol-contrib):
   - Extended set of components
   - Includes community-contributed receivers, processors, exporters
   - More features but larger binary

3. Distribution Builds:
   - Custom builds with specific components
   - Smaller binaries for specific use cases
   - Example: otelcol-loki (only Loki-related components)

Deployment Modes:
1. Agent Mode (Sidecar/DaemonSet):
   - Runs alongside applications
   - Receives data from local applications
   - Processes and forwards to central collector
   - Reduces network overhead

2. Gateway Mode (Standalone):
   - Centralized collector
   - Receives from multiple agents/applications
   - Processes and routes to backends
   - Handles authentication, routing, transformation

3. Hybrid Mode:
   - Agent collectors â†’ Gateway collector â†’ Backends
   - Best for large-scale deployments

Basic Collector Configuration:

  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  
  processors:
    batch:
      timeout: 1s
      send_batch_size: 1024
    memory_limiter:
      limit_mib: 512
    resource:
      attributes:
        - key: deployment.environment
          value: production
          action: upsert
  
  exporters:
    otlp/prometheus:
      endpoint: prometheus:9090
      tls:
        insecure: true
    logging:
      loglevel: debug
  
  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlp/prometheus, logging]
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlp/prometheus]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlp/prometheus]

Common Receivers:
- otlp: OpenTelemetry Protocol (gRPC and HTTP)
- prometheus: Scrape Prometheus metrics
- jaeger: Receive Jaeger traces
- zipkin: Receive Zipkin traces
- filelog: Read logs from files
- syslog: Receive syslog messages
- kafka: Consume from Kafka topics
- fluentforward: Receive Fluentd forward protocol

Common Processors:
- batch: Batch telemetry data before exporting
- memory_limiter: Prevent out-of-memory conditions
- resource: Add/modify resource attributes
- attributes: Add/modify span/metric/log attributes
- filter: Filter telemetry data based on conditions
- transform: Transform data using OTTL (OpenTelemetry Transformation Language)
- sampling: Sample traces based on conditions
- tail_sampling: Sample traces based on tail conditions
- span: Modify spans (rename, add attributes)
- metricstransform: Transform metrics

OPENTELEMETRY EXPORTERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What are Exporters?
- Exporters send telemetry data from applications or collectors to backends
- Available in both SDKs (application-level) and Collector (agent-level)
- Each exporter handles a specific protocol or backend format

Exporter Types:

1. OTLP Exporter (Recommended):
   - OpenTelemetry Protocol - the standard protocol
   - Supports gRPC and HTTP/JSON
   - Works with any OTLP-compatible backend
   - Most flexible and future-proof option

   Configuration (SDK):
     from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
     from opentelemetry.sdk.trace.export import BatchSpanProcessor
     
     otlp_exporter = OTLPSpanExporter(
         endpoint="http://collector:4317",
         insecure=True
     )
     span_processor = BatchSpanProcessor(otlp_exporter)

   Configuration (Collector):
     exporters:
       otlp/backend:
         endpoint: backend:4317
         tls:
           insecure: false
           cert_file: /path/to/cert.pem
         headers:
           api-key: "your-api-key"

2. Prometheus Exporter:
   - Exports metrics in Prometheus format
   - Exposes /metrics endpoint for Prometheus scraping
   - Only for metrics (not traces or logs)

   Configuration (SDK):
     from opentelemetry.exporter.prometheus import PrometheusMetricReader
     from opentelemetry.sdk.metrics import MeterProvider
     
     prometheus_reader = PrometheusMetricReader(port=9090)
     MeterProvider(metric_readers=[prometheus_reader])

   Configuration (Collector):
     exporters:
       prometheus:
         endpoint: "0.0.0.0:8889"
         const_labels:
           environment: production

3. Jaeger Exporter:
   - Exports traces to Jaeger backend
   - Supports gRPC and HTTP/Thrift protocols
   - Legacy but still widely used

   Configuration (SDK):
     from opentelemetry.exporter.jaeger.thrift import JaegerExporter
     
     jaeger_exporter = JaegerExporter(
         agent_host_name="localhost",
         agent_port=6831
     )

   Configuration (Collector):
     exporters:
       jaeger:
         endpoint: jaeger:14250
         tls:
           insecure: true

4. Zipkin Exporter:
   - Exports traces to Zipkin backend
   - HTTP/JSON format
   - Simple and lightweight

   Configuration (SDK):
     from opentelemetry.exporter.zipkin.json import ZipkinExporter
     
     zipkin_exporter = ZipkinExporter(
         endpoint="http://zipkin:9411/api/v2/spans"
     )

   Configuration (Collector):
     exporters:
       zipkin:
         endpoint: http://zipkin:9411/api/v2/spans
         format: json

5. Console Exporter:
   - Prints telemetry data to console/stdout
   - Useful for debugging and development
   - Not recommended for production

   Configuration (SDK):
     from opentelemetry.sdk.trace.export import ConsoleSpanExporter
     
     console_exporter = ConsoleSpanExporter()

   Configuration (Collector):
     exporters:
       logging:
         loglevel: debug
         sampling_initial: 5
         sampling_thereafter: 200

6. Loki Exporter:
   - Exports logs to Grafana Loki
   - Supports both push and pull modes
   - Part of Grafana ecosystem

   Configuration (Collector):
     exporters:
       loki:
         endpoint: http://loki:3100/loki/api/v1/push
         labels:
           resource:
             service.name: "service_name"
             service.namespace: "service_namespace"
         tenant_id: "tenant1"

7. Tempo Exporter:
   - Exports traces to Grafana Tempo
   - OTLP-compatible
   - Part of Grafana ecosystem

   Configuration (Collector):
     exporters:
       otlp/tempo:
         endpoint: tempo:4317
         tls:
           insecure: true

8. File Exporter:
   - Writes telemetry data to files
   - Useful for debugging and local development
   - Can be used for backup/archival

   Configuration (Collector):
     exporters:
       file:
         path: /var/log/otel/traces.json
         rotation:
           max_megabytes: 100
           max_days: 7

9. Kafka Exporter:
   - Exports to Kafka topics
   - Useful for high-throughput scenarios
   - Enables async processing

   Configuration (Collector):
     exporters:
       kafka:
         brokers:
           - kafka:9092
         topic: otel-traces
         encoding: otlp_proto

10. Datadog Exporter:
    - Exports to Datadog backend
    - Handles metrics, traces, and logs
    - Commercial observability platform

    Configuration (Collector):
      exporters:
        datadog:
          api:
            key: ${DD_API_KEY}
            site: datadoghq.com

11. New Relic Exporter:
    - Exports to New Relic backend
    - Commercial observability platform

    Configuration (Collector):
      exporters:
        newrelic:
          apikey: ${NEW_RELIC_API_KEY}
          timeout: 30s

12. Splunk Exporter:
    - Exports to Splunk backend
    - Supports HEC (HTTP Event Collector)

    Configuration (Collector):
      exporters:
        splunk_hec:
          endpoint: https://splunk:8088/services/collector
          token: ${SPLUNK_TOKEN}

Exporter Selection Guide:

For Grafana Stack:
  - Traces: OTLP exporter â†’ Tempo
  - Metrics: Prometheus exporter â†’ Prometheus/Mimir
  - Logs: Loki exporter â†’ Loki

For Cloud Providers:
  - AWS: AWS X-Ray exporter or OTLP â†’ CloudWatch
  - GCP: Google Cloud Monitoring exporter
  - Azure: Azure Monitor exporter

For Open Source:
  - OTLP exporter (most flexible)
  - Prometheus exporter (metrics)
  - Jaeger/Zipkin exporters (traces)

Best Practices for Exporters:

1. Use OTLP When Possible:
   - Standard protocol, vendor-neutral
   - Works with collector for routing
   - Future-proof

2. Batch Exports:
   - Use batch processors to reduce overhead
   - Configure appropriate batch sizes
   - Balance between latency and throughput

3. Error Handling:
   - Implement retry logic
   - Use circuit breakers
   - Don't let export failures break application

4. Authentication:
   - Use TLS for secure connections
   - Configure API keys/headers when required
   - Store credentials securely (env vars, secrets)

5. Performance:
   - Use async/background export
   - Monitor exporter performance
   - Adjust batch sizes based on load

6. Multiple Exporters:
   - Export to multiple backends via collector
   - Use collector for transformation
   - Avoid exporting directly from applications

Complete Example: Collector with Multiple Exporters

  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  
  processors:
    batch:
      timeout: 1s
      send_batch_size: 1024
    memory_limiter:
      limit_mib: 512
    resource:
      attributes:
        - key: deployment.environment
          value: production
          action: upsert
  
  exporters:
    # Export to Grafana Tempo
    otlp/tempo:
      endpoint: tempo:4317
      tls:
        insecure: true
    
    # Export to Prometheus
    prometheus:
      endpoint: "0.0.0.0:8889"
      const_labels:
        environment: production
    
    # Export to Loki
    loki:
      endpoint: http://loki:3100/loki/api/v1/push
      labels:
        resource:
          service.name: "service_name"
    
    # Debug logging
    logging:
      loglevel: info
  
  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [otlp/tempo, logging]
      metrics:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [prometheus, logging]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [loki, logging]

Best Practices:

1. Instrumentation Strategy:
   - Use auto-instrumentation when possible (less code, consistent)
   - Add manual instrumentation for business logic and custom metrics
   - Start with high-value services (user-facing, critical paths)
   - Gradually expand to all services

2. Sampling:
   - Implement sampling for high-volume traces (head-based or tail-based)
   - Sample 100% of errors, lower percentage of successful requests
   - Use probabilistic sampling (e.g., 10% of requests)
   - Example: Sample 100% errors, 10% successful requests

3. Resource Attributes:
   - Add consistent resource attributes (service.name, service.version)
   - Include deployment environment (dev, staging, prod)
   - Add infrastructure metadata (host, container, pod)
   - Use semantic conventions for standard attributes

4. Naming Conventions:
   - Follow semantic conventions for spans and metrics
   - Use consistent naming across services
   - Example: http.method, http.status_code, db.operation

5. Context Propagation:
   - Ensure trace context propagates across service boundaries
   - Use W3C Trace Context standard
   - Propagate context in HTTP headers, gRPC metadata, message queues

6. Performance Considerations:
   - Use async/background export to avoid blocking application
   - Batch telemetry data before exporting
   - Set appropriate batch sizes and timeouts
   - Monitor collector resource usage

7. Error Handling:
   - Don't let telemetry failures break your application
   - Use circuit breakers for exporters
   - Log telemetry errors separately
   - Have fallback mechanisms

8. Security:
   - Encrypt telemetry data in transit (TLS)
   - Use authentication for collector endpoints
   - Sanitize sensitive data (PII, passwords) before export
   - Use processors to redact sensitive information

9. Cost Management:
   - Use sampling to reduce data volume
   - Filter unnecessary telemetry at the collector
   - Use processors to drop low-value data
   - Monitor telemetry data volume and costs

10. Integration with Observability Stack:
    - Use OTLP exporter for Grafana Tempo (traces)
    - Use Prometheus exporter for metrics
    - Use Loki exporter for logs
    - Leverage OpenTelemetry Collector for routing

Common Patterns:

Pattern 1: Simple Application Instrumentation
  from opentelemetry import trace
  from opentelemetry.sdk.trace import TracerProvider
  from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor
  
  trace.set_tracer_provider(TracerProvider())
  tracer = trace.get_tracer(__name__)
  
  with tracer.start_as_current_span("my_operation"):
      # Your code here
      pass

Pattern 2: Distributed Tracing
  # Service A calls Service B
  with tracer.start_as_current_span("service_a_operation") as span:
      headers = {}
      trace.inject(headers)  # Inject trace context
      response = requests.get("http://service-b/api", headers=headers)

Pattern 3: Custom Metrics
  from opentelemetry import metrics
  from opentelemetry.sdk.metrics import MeterProvider
  
  meter = metrics.get_meter(__name__)
  counter = meter.create_counter("requests_total")
  counter.add(1, {"method": "GET", "status": "200"})

Pattern 4: Log Correlation
  import logging
  from opentelemetry import trace
  
  logger = logging.getLogger(__name__)
  span = trace.get_current_span()
  trace_id = format(span.get_span_context().trace_id, '032x')
  logger.info(f"Processing request [trace_id={trace_id}]")

Integration with Grafana Stack:
- Traces â†’ Grafana Tempo (via OTLP)
- Metrics â†’ Prometheus/Mimir (via Prometheus exporter)
- Logs â†’ Loki (via Loki exporter)
- Visualization â†’ Grafana (unified dashboards)

Key Benefits:
1. Vendor Neutrality: Switch backends without code changes
2. Standardization: Consistent instrumentation across languages
3. Rich Context: Automatic context propagation
4. Auto-Instrumentation: Minimal code changes required
5. Community Support: Large ecosystem and active development
6. Future-Proof: Industry standard for observability
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROMETHEUS REMOTE WRITE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Remote Write?
- Remote write is a Prometheus feature that allows writing metrics to remote 
  storage systems in real-time
- Prometheus continuously sends collected metrics to remote endpoints via HTTP POST
- Works alongside local storage (doesn't replace it, but can be used for long-term 
  retention or backup)
- Uses the Prometheus remote write protocol (protobuf over HTTP)

Key Characteristics:
- Asynchronous: Writes happen in background, doesn't block scraping
- Reliable: Includes retry logic and queuing for failed writes
- Efficient: Batches metrics before sending
- Configurable: Multiple remote write endpoints can be configured
- Supports relabeling: Can modify metrics before sending

Why Use Remote Write?
1. Long-Term Storage:
   - Prometheus local storage is limited (typically 15-30 days)
   - Remote write enables long-term retention (months/years)
   - Store historical data in cost-effective storage

2. Centralized Storage:
   - Aggregate metrics from multiple Prometheus instances
   - Single source of truth for metrics across environments
   - Easier to query and analyze

3. High Availability:
   - Backup metrics to remote storage
   - Recover from Prometheus instance failures
   - Redundancy for critical metrics

4. Scalability:
   - Offload storage from Prometheus
   - Use scalable storage backends (S3, object storage)
   - Handle high-cardinality metrics

5. Multi-Tenancy:
   - Send metrics to different backends per tenant
   - Isolate metrics by environment or team
   - Route metrics based on labels

How It Works:
1. Prometheus collects metrics (scraping)
2. Metrics are stored locally (TSDB)
3. Remote write process batches metrics
4. Batches are sent to remote endpoint via HTTP POST
5. Remote storage receives and stores metrics
6. Failed writes are queued and retried

Architecture:
  Prometheus â†’ Local TSDB â†’ Remote Write Queue â†’ Remote Storage Backend
                    â†“              â†“
              (Scraping)    (Batching & Sending)

Basic Configuration:

  global:
    scrape_interval: 15s
  
  remote_write:
    - url: "https://remote-storage/api/v1/write"
      queue_config:
        max_samples_per_send: 1000
        batch_send_deadline: 5s
        max_retries: 3
        min_backoff: 30ms
        max_backoff: 100ms

Configuration Options:

1. URL (Required):
   - Endpoint where metrics will be sent
   - Must support Prometheus remote write protocol
   - Example: "https://prometheus-remote-write:9090/api/v1/write"

2. Queue Configuration:
   remote_write:
     - url: "https://remote-storage/api/v1/write"
       queue_config:
         max_samples_per_send: 1000    # Max samples per batch
         batch_send_deadline: 5s       # Max time before sending batch
         max_retries: 3                 # Retry attempts
         min_backoff: 30ms              # Min delay between retries
         max_backoff: 100ms             # Max delay between retries
         capacity: 10000                # Queue capacity

3. Write Relabeling:
   remote_write:
     - url: "https://remote-storage/api/v1/write"
       write_relabel_configs:
         - source_labels: [__name__]
           regex: 'expensive_metric.*'
           action: drop                 # Drop expensive metrics
         - source_labels: [environment]
           regex: 'dev'
           action: drop                 # Don't send dev metrics
         - source_labels: [job]
           target_label: service        # Rename label

4. Authentication:
   remote_write:
     - url: "https://remote-storage/api/v1/write"
       basic_auth:
         username: "prometheus"
         password: "secret"
       # OR
       bearer_token: "token-here"
       # OR
       bearer_token_file: "/path/to/token"
       # OR
       oauth2:
         client_id: "prometheus"
         client_secret: "secret"
         token_url: "https://auth-server/oauth/token"

5. TLS Configuration:
   remote_write:
     - url: "https://remote-storage/api/v1/write"
       tls_config:
         ca_file: "/path/to/ca.crt"
         cert_file: "/path/to/client.crt"
         key_file: "/path/to/client.key"
         insecure_skip_verify: false

6. Multiple Remote Write Endpoints:
   remote_write:
     - url: "https://mimir/api/v1/write"
       write_relabel_configs:
         - source_labels: [environment]
           regex: 'production'
           action: keep
   
     - url: "https://thanos/api/v1/write"
       write_relabel_configs:
         - source_labels: [environment]
           regex: 'staging'
           action: keep

Common Remote Write Backends:

1. Prometheus Mimir:
   - Long-term storage for Prometheus
   - Horizontally scalable
   - Multi-tenancy support
   - Configuration:
     remote_write:
       - url: "http://mimir:9009/api/v1/push"
         headers:
           X-Scope-OrgID: "tenant-1"

2. Grafana Cloud:
   - Managed Prometheus service
   - Configuration:
     remote_write:
       - url: "https://prometheus-prod-01-prod-us-central-0.grafana.net/api/prom/push"
         basic_auth:
           username: "your-username"
           password: "your-api-key"

3. Thanos:
   - Long-term storage and query layer
   - Configuration:
     remote_write:
       - url: "http://thanos-receive:10908/api/v1/receive"

4. Cortex:
   - Horizontally scalable Prometheus
   - Configuration:
     remote_write:
       - url: "http://cortex:9009/api/v1/push"

5. VictoriaMetrics:
   - High-performance time series database
   - Configuration:
     remote_write:
       - url: "http://victoriametrics:8428/api/v1/write"

6. AWS Managed Service for Prometheus:
   - AWS managed Prometheus service
   - Configuration:
     remote_write:
       - url: "https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-xxx/api/v1/remote_write"
         sigv4:
           region: us-east-1

7. Google Cloud Monitoring:
   - GCP managed monitoring
   - Configuration:
     remote_write:
       - url: "https://monitoring.googleapis.com/v1/projects/PROJECT_ID/location/global/prometheus"
         oauth2:
           client_id: "client-id"
           client_secret: "secret"
           token_url: "https://oauth2.googleapis.com/token"

Use Cases:

1. Long-Term Retention:
   - Store metrics for compliance (1+ years)
   - Historical analysis and trending
   - Cost-effective storage (S3, object storage)

2. Multi-Prometheus Aggregation:
   - Multiple Prometheus instances â†’ Single storage
   - Global view across regions/environments
   - Centralized querying

3. Backup and Disaster Recovery:
   - Backup critical metrics
   - Recover from Prometheus failures
   - Redundancy for high availability

4. Cost Optimization:
   - Move old data to cheaper storage
   - Keep hot data in Prometheus, cold data remote
   - Reduce Prometheus storage requirements

5. Multi-Tenancy:
   - Route metrics by tenant/environment
   - Isolate metrics per team
   - Different retention per tenant

6. Cloud Migration:
   - Send metrics to cloud-managed services
   - Hybrid cloud observability
   - Multi-cloud strategies

Best Practices:

1. Queue Configuration:
   - Set appropriate batch sizes (1000-2000 samples)
   - Configure reasonable timeouts (5-10s)
   - Balance between latency and throughput
   - Monitor queue length and drops

2. Relabeling:
   - Drop unnecessary metrics before sending
   - Reduce cardinality (drop high-cardinality labels)
   - Rename labels for consistency
   - Filter by environment/importance

3. Authentication:
   - Always use authentication (basic auth, bearer token, OAuth2)
   - Use TLS for encrypted communication
   - Rotate credentials regularly
   - Store secrets securely

4. Monitoring:
   - Monitor remote write success rate
   - Alert on write failures
   - Track queue length and drops
   - Monitor network bandwidth usage

5. Error Handling:
   - Configure appropriate retries
   - Set reasonable backoff times
   - Monitor and alert on persistent failures
   - Have fallback mechanisms

6. Performance:
   - Use multiple remote write endpoints for load distribution
   - Filter metrics to reduce volume
   - Optimize batch sizes
   - Monitor Prometheus resource usage

7. Cost Management:
   - Filter expensive metrics
   - Use different retention per metric type
   - Route to cost-appropriate storage
   - Monitor data volume and costs

Example: Complete Configuration

  global:
    scrape_interval: 15s
    external_labels:
      cluster: production
      region: us-east-1
  
  remote_write:
    # Production metrics to Mimir
    - url: "http://mimir:9009/api/v1/push"
      queue_config:
        max_samples_per_send: 1000
        batch_send_deadline: 5s
        max_retries: 3
        capacity: 10000
      write_relabel_configs:
        - source_labels: [__name__]
          regex: 'go_.*|prometheus_.*'
          action: drop
        - source_labels: [environment]
          regex: 'production'
          action: keep
      basic_auth:
        username: "prometheus"
        password_file: "/etc/prometheus/remote-write-password"
    
    # Backup to Thanos
    - url: "http://thanos-receive:10908/api/v1/receive"
      queue_config:
        max_samples_per_send: 2000
        batch_send_deadline: 10s
      write_relabel_configs:
        - source_labels: [__name__]
          regex: 'critical_.*'
          action: keep  # Only backup critical metrics

Monitoring Remote Write:

Key Metrics to Monitor:
- prometheus_remote_storage_succeeded_samples_total
- prometheus_remote_storage_failed_samples_total
- prometheus_remote_storage_queue_samples_pending
- prometheus_remote_storage_queue_samples_dropped_total
- prometheus_remote_storage_sent_batch_duration_seconds

Example Alert:
  - alert: RemoteWriteFailures
    expr: rate(prometheus_remote_storage_failed_samples_total[5m]) > 0
    for: 5m
    annotations:
      summary: "Remote write failures detected"

Troubleshooting:

Common Issues:
1. Authentication failures â†’ Check credentials
2. Network timeouts â†’ Check connectivity and firewall
3. Queue full â†’ Increase capacity or reduce metric volume
4. High latency â†’ Optimize batch configuration
5. Dropped samples â†’ Check relabeling rules

Debug Commands:
  # Check remote write status
  curl http://localhost:9090/api/v1/status/config
  
  # Check queue metrics
  curl http://localhost:9090/api/v1/query?query=prometheus_remote_storage_queue_samples_pending

Key Takeaways:
âœ“ Remote write enables long-term storage and backup
âœ“ Configure appropriate queue settings for your workload
âœ“ Use relabeling to filter and reduce metric volume
âœ“ Always use authentication and TLS
âœ“ Monitor remote write health and performance
âœ“ Multiple endpoints allow routing and redundancy
âœ“ Remote write works alongside local storage (not replacement)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GRAFANA ALLOY - SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

What is Grafana Alloy?
- Vendor-neutral, open-source observability agent
- Successor to Grafana Agent, built on OpenTelemetry Collector framework
- Collects and forwards metrics, logs, and traces to various backends
- Uses River configuration language (declarative, type-safe)
- Single binary for easy deployment and management

Key Features:
- Unified agent for metrics, logs, and traces
- Component-based architecture (modular and extensible)
- Supports multiple protocols (Prometheus, OTLP, Loki, etc.)
- Service discovery for dynamic targets
- Built-in relabeling and transformation
- Multi-backend routing (send same data to multiple destinations)

Core Capabilities:
1. Metrics Collection:
   - Scrape Prometheus metrics from applications
   - Forward to Prometheus, Mimir, or other backends
   - Service discovery (Kubernetes, Consul, AWS, etc.)
   - Metrics relabeling and transformation

2. Log Collection:
   - Collect from files, syslog, journald, Docker
   - Forward to Loki or other log systems
   - Log parsing and transformation
   - Multi-line log handling

3. Distributed Tracing:
   - Collect traces via OTLP, Jaeger, Zipkin
   - Forward to Tempo or other trace backends
   - Trace sampling and filtering
   - Service map generation

Use Cases:

1. Unified Observability Agent:
   - Single agent for all telemetry types (metrics, logs, traces)
   - Reduces operational overhead
   - Consistent configuration across environments
   - Easier to manage than multiple agents

2. Kubernetes Observability:
   - Deploy as DaemonSet for node-level collection
   - Use as sidecar for pod-level collection
   - Automatic service discovery for pods/services
   - Kubernetes label injection

3. Multi-Backend Routing:
   - Route same data to multiple backends simultaneously
   - Different retention policies per backend
   - Backup and redundancy strategies
   - Multi-cloud observability

4. Edge/Remote Monitoring:
   - Deploy at edge locations
   - Collect data from remote sites
   - Forward to central observability stack
   - Bandwidth optimization with filtering

5. Prometheus Replacement/Enhancement:
   - Can replace Prometheus for scraping
   - Remote write to Prometheus/Mimir
   - More flexible than Prometheus-only setup
   - Better for multi-tenant scenarios

6. Log Shipping Alternative:
   - Alternative to Promtail for Loki
   - More features than Promtail (multi-backend, transformation)
   - Can handle metrics and traces too
   - Better for complex routing needs

7. OpenTelemetry Integration:
   - Native OTLP support
   - Works with OpenTelemetry Collector components
   - Standard observability protocol
   - Future-proof architecture

Installation:

# Using Homebrew (macOS)
brew install grafana-alloy

# Using Docker
docker pull grafana/alloy:latest

# Download binary
wget https://github.com/grafana/alloy/releases/download/v1.0.0/alloy-1.0.0-darwin-amd64.tar.gz
tar -xzf alloy-1.0.0-darwin-amd64.tar.gz
./alloy --config.file=config.river

# Kubernetes (using Helm)
helm repo add grafana https://grafana.github.io/helm-charts
helm install alloy grafana/alloy

Basic Configuration Example (River):

  // Prometheus metrics scraping
  prometheus.scrape "app_metrics" {
    targets = [
      {
        __address__ = "localhost:8080",
        job = "myapp",
      },
    ]
    forward_to = [prometheus.remote_write.mimir.receiver]
  }

  // Remote write to Mimir
  prometheus.remote_write "mimir" {
    endpoint {
      url = "http://mimir:9009/api/v1/push"
      basic_auth {
        username = env("MIMIR_USERNAME")
        password = env("MIMIR_PASSWORD")
      }
    }
  }

  // Log collection
  loki.source.file "app_logs" {
    targets = [
      {
        __path__ = "/var/log/app/*.log",
        job = "myapp",
      },
    ]
    forward_to = [loki.write.loki.receiver]
  }

  // Forward logs to Loki
  loki.write "loki" {
    endpoint {
      url = "http://loki:3100/loki/api/v1/push"
    }
  }

  // OTLP receiver for traces
  otelcol.receiver.otlp "traces" {
    grpc {
      endpoint = "0.0.0.0:4317"
    }
    http {
      endpoint = "0.0.0.0:4318"
    }
    output {
      traces = [otelcol.exporter.otlp.tempo.input]
    }
  }

  // Export traces to Tempo
  otelcol.exporter.otlp "tempo" {
    endpoint = "tempo:4317"
    tls {
      insecure = true
    }
  }

Best Practices:

1. Configuration Management:
   âœ“ Use River configuration files (not YAML)
   âœ“ Version control all Alloy configurations
   âœ“ Use environment variables for sensitive data
   âœ“ Modularize configurations for reusability
   âœ“ Use River modules for shared components

2. Deployment Patterns:
   âœ“ Use DaemonSet for node-level collection in Kubernetes
   âœ“ Use sidecar pattern for application-specific collection
   âœ“ Deploy as systemd service on VMs
   âœ“ Use Docker Compose for local development
   âœ“ Consider resource limits in containerized environments

3. Resource Management:
   âœ“ Set appropriate memory limits
   âœ“ Configure batch sizes for efficient processing
   âœ“ Use sampling for high-volume traces
   âœ“ Filter unnecessary data early in pipeline
   âœ“ Monitor Alloy's own metrics

4. Security:
   âœ“ Use TLS for all backend connections
   âœ“ Store credentials in environment variables or secrets
   âœ“ Use authentication for all backends
   âœ“ Restrict network access (firewall rules)
   âœ“ Rotate credentials regularly

5. Performance Optimization:
   âœ“ Batch data before sending to backends
   âœ“ Use relabeling to reduce cardinality
   âœ“ Filter data at source when possible
   âœ“ Configure appropriate scrape intervals
   âœ“ Monitor queue lengths and drops

6. Service Discovery:
   âœ“ Use Kubernetes service discovery for dynamic targets
   âœ“ Leverage Consul, AWS, or file-based discovery
   âœ“ Configure appropriate refresh intervals
   âœ“ Handle discovery failures gracefully

7. Error Handling:
   âœ“ Configure retry logic for failed writes
   âœ“ Monitor Alloy health endpoints
   âœ“ Set up alerts for collection failures
   âœ“ Use dead letter queues for failed data
   âœ“ Log errors for troubleshooting

8. Multi-Backend Strategy:
   âœ“ Route critical data to multiple backends
   âœ“ Use different retention per backend
   âœ“ Filter data per backend (reduce costs)
   âœ“ Test failover scenarios

9. Monitoring Alloy:
   âœ“ Expose Alloy's own metrics endpoint
   âœ“ Monitor collection success rates
   âœ“ Track queue lengths and drops
   âœ“ Alert on configuration errors
   âœ“ Monitor resource usage

10. Migration from Grafana Agent:
    âœ“ Alloy uses River (not YAML like Agent)
    âœ“ Convert existing Agent configs to River
    âœ“ Test configurations in staging first
    âœ“ Gradual migration approach recommended
    âœ“ Alloy is backward compatible with Agent's functionality

Alloy vs Other Tools:

Alloy vs Promtail:
- Alloy: Unified agent (metrics, logs, traces)
- Promtail: Log-only shipper
- Use Alloy when you need metrics/traces too
- Use Promtail for simple log-only scenarios

Alloy vs Prometheus:
- Alloy: Agent/collector (scrapes and forwards)
- Prometheus: Time-series database with query engine
- Alloy can scrape and forward to Prometheus
- Alloy doesn't replace Prometheus, complements it

Alloy vs OpenTelemetry Collector:
- Alloy: Built on OTel Collector framework
- Similar capabilities, different config language
- Alloy uses River, OTel Collector uses YAML
- Alloy is Grafana-optimized

Common Patterns:

Pattern 1: Kubernetes DaemonSet
  - Deploy Alloy as DaemonSet
  - Scrape node-level metrics
  - Collect container logs
  - Forward to central backends

Pattern 2: Application Sidecar
  - Deploy Alloy as sidecar container
  - Collect application-specific telemetry
  - Forward to backends
  - Isolated per application

Pattern 3: Edge Collection
  - Deploy Alloy at edge locations
  - Collect local telemetry
  - Forward to central stack
  - Handle network failures gracefully

Pattern 4: Multi-Tenant Routing
  - Route data by tenant labels
  - Different backends per tenant
  - Isolated data pipelines
  - Cost optimization per tenant

Key Benefits:
1. Unified Agent: Single agent for all telemetry types
2. Vendor Neutral: Works with any backend
3. Flexible: Component-based architecture
4. Modern: Built on OpenTelemetry standards
5. Efficient: Optimized for resource usage
6. Scalable: Handles high-volume data
7. Maintainable: River language is type-safe

When to Use Alloy:
âœ“ Need unified agent for metrics, logs, and traces
âœ“ Want to replace multiple agents (Promtail, exporters)
âœ“ Need flexible routing to multiple backends
âœ“ Require advanced transformation capabilities
âœ“ Building modern observability stack
âœ“ Using Grafana ecosystem (Loki, Tempo, Mimir)

When NOT to Use Alloy:
âœ— Simple log-only scenarios (Promtail is simpler)
âœ— Already have working Prometheus-only setup
âœ— Don't need traces or advanced features
âœ— Very resource-constrained environments
âœ— Prefer YAML over River configuration

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•



Things to practice
  - Implement Side Car patern
  - implement or configure these in AWS cloud both in EC2 and ECS , if there are any other managed services there  as well
  - Relabeling K8 values : https://www.youtube.com/watch?v=mXvuKAb6ZwY
  - Use Service discovery with AWS and file basesd Service discovery
  - Use All the available authentication techniques to secure all the components of Promethus (alert manager, exporter, UI, push gateway etc)
  - enable https 
  - application or packages ecuroty and vulnerablity checks or CVE checks
  - learn about  loki, tempo, mimir, pyroscope,beyla, alloy, k6 ,opentelemetry
  - Enable to log distrubted tracing and how can we follow and log it 
  - Introduce chaos engineering with logs 
  - implement use grafana , loki, tempo , alloy . all on loki
  - Create diagram with visual explanation for all component
    - grafana
    - Prometheus
    - Loki
    - Service discovery
    - alert manager
    - node exporters
    - Alloy
    - Promtail
    - push gateway




  - learn Database engineering and patterns


